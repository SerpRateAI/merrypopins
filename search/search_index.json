{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Merrypopins","text":"<p>merrypopins is a Python library to streamline the workflow of nano\u2011indentation experiment data processing, automated pop-in detection and analysis. It provides five core modules:</p> <ul> <li><code>load_datasets</code>: Load and parse <code>.txt</code> measurement files and <code>.tdm</code>/<code>.tdx</code> metadata files into structured pandas DataFrames. Automatically detects headers, timestamps, and measurement channels.</li> <li><code>preprocess</code>: Clean and normalize indentation data with filtering, baseline correction, and contact point detection.</li> <li><code>locate</code>: Identify and extract pop\u2011in events within indentation curves using advanced detection algorithms, including:</li> <li>Isolation Forest anomaly detection</li> <li>CNN Autoencoder reconstruction error</li> <li>Fourier-based derivative outlier detection</li> <li>Savitzky-Golay smoothed gradient thresholds</li> <li><code>statistics</code>: Perform statistical analysis and model fitting on located pop\u2011in events (e.g., frequency, magnitude, distribution). The statistics module allows you to compute detailed pop-in statistics, such as:</li> <li>Pop-in statistics (e.g., load-depth and stress-strain metrics)</li> <li>Stress-strain transformation using Kalidindi &amp; Pathak. (2008)</li> <li>Curve-level summary statistics (e.g., total pop-in duration, average time between pop-ins)</li> <li>Pop-in shape statistics like depth jump, average velocity, and curvature</li> <li><code>make_dataset</code>: Construct enriched datasets by running the full merrypopins pipeline and exporting annotated results and visualizations.</li> </ul>"},{"location":"#try-merrypopins-library-online","title":"\ud83c\udf10 Try Merrypopins Library Online","text":"<p>\ud83d\ude80 Live demo: explore Merrypopins in your browser! </p> <p>The hosted app lets you:</p> <ul> <li>upload raw <code>.txt</code> indentation files (and optional <code>.tdm/.tdx</code> metadata),</li> <li>tune preprocessing, detection &amp; statistics parameters,</li> <li>visualise pop-ins interactively,</li> <li>download annotated CSVs + plots.</li> </ul>"},{"location":"#source-instrumentation","title":"\ud83d\udee0 Source Instrumentation","text":"<p>Merrypopins was developed using datasets generated by the Bruker Hysitron TI 990 TriboIndenter \u2014 a high-precision nanoindentation platform. The library natively supports .txt and .tdm/.tdx file formats exported by the Hysitron software suite.</p> <p>Typical indentation experiments conducted with the TI 990 include:</p> <ul> <li>Force-depth curve acquisition at nano/micro scale</li> <li>High-resolution pop-in event detection</li> <li>Automated test grid data export</li> </ul> <p>The preprocessing and pop-in detection tools in Merrypopins are tuned to handle the structural patterns and noise profiles specific to these datasets.</p>"},{"location":"#example-nanoindentation-grain-selection-and-deformation","title":"Example: Nanoindentation Grain Selection and Deformation","text":"<p>Below are example visualizations from Electron Backscatter Diffraction (EBSD) maps used to select grain areas, followed by indentation marks after testing:</p>"},{"location":"#pre-indentation-ebsd-with-labeled-grains","title":"\u27a4 Pre-indentation EBSD with Labeled Grains","text":""},{"location":"#post-indentation-microstructure-with-deformation-area-on-grain-5","title":"\u27a4 Post-indentation Microstructure with Deformation (Area on Grain 5)","text":"<p>These images highlight the complex deformation behavior analyzed by the <code>merrypopins</code> toolset for robust pop-in detection.</p> <p>For a quick overview, see the Quickstart.</p> <p>Merrypopins is developed by Cahit Acar, Anna Marcelissen, Hugo van Schrojenstein Lantman, and John M. Aiken.</p>"},{"location":"RELEASING/","title":"\ud83d\udce6 Releasing Merrypopins","text":"<p>This document describes the steps to release a new version of the <code>merrypopins</code> Python package to both PyPI and GitHub Releases.</p>"},{"location":"RELEASING/#requirements","title":"\u2705 Requirements","text":"<ul> <li>You have write access to the repository.</li> <li>A valid <code>PYPI_API_TOKEN</code> is stored in GitHub Secrets.</li> <li>Versioning follows Semantic Versioning (e.g. <code>0.3.0</code>).</li> </ul>"},{"location":"RELEASING/#release-checklist","title":"\ud83d\udd01 Release Checklist","text":""},{"location":"RELEASING/#1-update-the-version","title":"1. Update the Version","text":"<p>Update version number in:</p> <ul> <li><code>pyproject.toml</code>:   <pre><code>[project]\nversion = \"0.3.0\"\n</code></pre></li> <li><code>src/merrypopins/__init__.py</code>:   <pre><code>__version__ = \"0.3.0\"\n</code></pre></li> </ul> <p>\u26a0\ufe0f Both values must match.</p>"},{"location":"RELEASING/#2-update-the-changelog-optional","title":"2. Update the Changelog (Optional)","text":"<p>In the <code>CHANGELOG.md</code>, add a section for the new version:</p> <pre><code>## [0.3.0] - 2025-06-06\n- Improved Fourier detection accuracy\n- Fixed CNN trimming\n- Automated release via GitHub Actions\n</code></pre>"},{"location":"RELEASING/#3-merge-to-main-branch","title":"3. Merge to Main Branch","text":"<p>Create a pull request to merge your changes into the release branch (<code>main</code>).</p> <p>Once merged, the GitHub Actions workflow will:</p> <ul> <li>Validate version consistency</li> <li>Build the package</li> <li>Publish it to PyPI</li> <li>Tag the release (<code>v0.3.0</code>)</li> <li>Create a GitHub release with autogenerated notes</li> </ul>"},{"location":"RELEASING/#verification-steps","title":"\ud83d\udd0d Verification Steps","text":"<ul> <li>\u2705 Go to PyPI and check that the version is published.</li> <li>\u2705 Go to GitHub Releases and confirm the tag and release notes were created.</li> </ul>"},{"location":"RELEASING/#notes","title":"\ud83e\udde0 Notes","text":"<ul> <li>The workflow ensures versions are only released once.</li> <li>Version mismatch between <code>pyproject.toml</code> and <code>__init__.py</code> will fail the release.</li> <li>Auto-generated release notes summarize commit history since the last tag.</li> </ul>"},{"location":"RELEASING/#related-files","title":"\ud83d\udcc1 Related Files","text":"<ul> <li><code>pyproject.toml</code></li> <li><code>src/merrypopins/__init__.py</code></li> <li><code>.github/workflows/pypi-release.yml</code></li> <li><code>.github/RELEASING.md</code></li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please file issues and submit pull requests on GitHub.</p>"},{"location":"contributing/#branching-model","title":"Branching Model","text":"<p>main \u2190 \ud83d\udce6 production releases dev \u2190 \ud83d\udee0 active development (default Pull Request target)</p> <ul> <li><code>main</code> holds only stable, version-tagged releases.  </li> <li><code>dev</code> is the rolling integration branch where all feature / fix PRs land first.   Maintainers periodically open an internal PR from <code>dev</code> \u2192 <code>main</code> when a new   release is ready.</li> </ul>"},{"location":"contributing/#how-to-open-a-pull-request","title":"How to Open a Pull Request","text":"<ol> <li>Fork the repository to your GitHub account.  </li> <li>Clone your fork and set the upstream remote: <pre><code>git clone https://github.com/&lt;your-user&gt;/merrypopins.git\ncd merrypopins\ngit remote add upstream https://github.com/SerpRateAI/merrypopins.git\n</code></pre></li> <li>Sync &amp; branch off <code>dev</code>:    <pre><code>git fetch upstream\ngit checkout -b feature/awesome upstream/dev\n</code></pre></li> <li>Do your work \u2192 commit:    <pre><code>git commit -m \"feat: add awesome feature\"\n</code></pre></li> <li>Push to your fork:    <pre><code>git push origin feature/awesome\n</code></pre></li> <li>Open a pull request into <code>dev</code> (set the PR\u2019s base branch to <code>dev</code>).  </li> <li>Address any review comments &amp; keep your feature branch updated with the latest <code>dev</code> if needed.  </li> </ol> <p>Note: Once your PR is merged into <code>dev</code>, the maintainers will handle promoting <code>dev</code> to <code>main</code> when preparing a new release\u2014please don\u2019t open PRs directly against <code>main</code>.</p> <p>This project is licensed under the MIT see LICENSE.</p>"},{"location":"documentation/","title":"\ud83e\uddea Experimental Metadata Reference","text":"<p>This section documents important metadata fields collected during nanoindentation experiments using the Hysitron TI 990 system. These variables are useful for preprocessing raw data, validating test conditions, and performing downstream statistical analysis.</p>"},{"location":"documentation/#metadata-variables-explained","title":"\ud83d\udcd0 Metadata Variables Explained","text":""},{"location":"documentation/#acquisition_contact_threshold__n__","title":"<code>Acquisition_Contact_Threshold__\u03bcN__</code>","text":"<ul> <li>Description: Force threshold at which the machine determines that contact has been made.</li> <li>Use case: Important for preprocessing. The region before this threshold typically contains fake contact cycles and should be trimmed.</li> <li>Note: Marks the true start of the indentation curve.</li> </ul>"},{"location":"documentation/#initial_drift_rate__nm___s__","title":"<code>Initial_Drift_Rate__nm___s__</code>","text":"<ul> <li>Description: Rate of displacement drift while the tip is held stationary at the start of the experiment.</li> <li>Use case: Useful for assessing environmental stability and potential corrections during analysis.</li> <li>Related field: <code>Acquisition_Drift_Monitor_Time__s__</code> specifies how long drift was measured.</li> <li>Insight: High drift rates may indicate unstable conditions (e.g., thermal fluctuations).</li> </ul>"},{"location":"documentation/#acquisition_lift_height__nm__","title":"<code>Acquisition_Lift_Height__nm__</code>","text":"<ul> <li>Description: Distance the indenter tip is lifted away from the sample before initiating the actual indentation.</li> <li>Use case: Helps interpret displacement values below 0 nm in raw data, which should be trimmed during preprocessing.</li> </ul>"},{"location":"documentation/#acquisition_maximum_demand__n__","title":"<code>Acquisition_Maximum_Demand__\u00b5N__</code>","text":"<ul> <li>Description: Maximum force applied during the indentation.</li> <li>Use case: Critical for verifying test configuration:</li> <li>6\u202f\u00b5m tip \u2192 ~200 mN</li> <li>Note: Inconsistent maximum demand values may indicate incorrect experimental parameters.</li> </ul>"},{"location":"documentation/#current_area_function_b-c0-c1-c5","title":"<code>Current_Area_Function_B</code>, <code>C0</code>, <code>C1</code>, ..., <code>C5</code>","text":"<ul> <li>Description: Polynomial coefficients used to model the contact area of the tip as a function of depth:</li> </ul> <p>[   A(h) = C_0 h^2 + C_1 h + C_2 h^{0.5} + \\dots + C_5 h^{-2}   ]</p> <ul> <li>Use case: Necessary for accurate calculation of hardness and stress.</li> <li>Explanation: The shape of the indenter tip may deviate from ideal geometry; these coefficients correct for that.</li> </ul>"},{"location":"documentation/#current_tip_modulus__mpa__","title":"<code>Current_Tip_Modulus__MPa__</code>","text":"<ul> <li>Description: Young\u2019s modulus of the indenter tip material.</li> <li>Use case: Required in modulus and hardness calculations (e.g., reduced modulus).</li> </ul>"},{"location":"documentation/#current_tip_poissons_ratio","title":"<code>Current_Tip_Poissons_Ratio</code>","text":"<ul> <li>Description: Poisson\u2019s ratio of the tip.</li> <li>Use case: Used in stress-strain calculations and corrections based on contact mechanics.</li> </ul>"},{"location":"documentation/#summary-table","title":"\ud83d\udd0d Summary Table","text":"Variable Unit Purpose <code>Acquisition_Contact_Threshold__\u03bcN__</code> \u03bcN Trim pre-contact region <code>Initial_Drift_Rate__nm___s__</code> nm/s Environmental quality check <code>Acquisition_Lift_Height__nm__</code> nm Detect trimmed offset region <code>Acquisition_Maximum_Demand__\u00b5N__</code> \u03bcN Validate experiment configuration <code>Current_Area_Function_B</code>, <code>C0..C5</code> varies Calibrate contact area <code>Current_Tip_Modulus__MPa__</code> MPa Used in modulus calculation <code>Current_Tip_Poissons_Ratio</code> dimensionless Stress-strain relations"},{"location":"documentation/#additional-notes","title":"\ud83e\uddf0 Additional Notes","text":"<ul> <li>All metadata fields are parsed from the <code>.tdm</code> files exported by Hysitron's software.</li> <li>These fields can be accessed using <code>load_tdm()</code> in the <code>merrypopins.load_datasets</code> module.</li> <li>Consider saving metadata alongside each test\u2019s main <code>.txt</code> dataset for reproducibility and auditing.</li> </ul>"},{"location":"documentation/#experimental-setup-and-file-naming-conventions","title":"\ud83d\udd2c Experimental Setup and File Naming Conventions","text":"<p>This page outlines the experimental equipment, procedure, file naming scheme, and specific loading parameters used in the indentation experiments analyzed by merrypopins.</p>"},{"location":"documentation/#instrumentation","title":"\ud83e\uddea Instrumentation","text":"<p>All nanoindentation experiments were performed using the Bruker Hysitron TI 990 TriboIndenter, a precision instrument for measuring mechanical properties at the nanoscale.</p> <ul> <li>Transducer: 3D OMNIProbe</li> <li>Max Load: 10 N</li> <li>Max Displacement: 80 \u00b5m</li> <li>Displacement Resolution: &lt; 0.01 nm</li> <li>Displacement Noise Floor: 0.5 nm</li> <li>Force Resolution: 0.0014 mN</li> <li>Force Noise Floor: 0.07 mN</li> <li>Control Software: TriboScan 12.0.0.391</li> <li>Tip Type: Conospherical diamond</li> <li>6 \u00b5m tip actual radius: 5.323 \u00b5m</li> </ul> <p>Calibration Materials: - Fused Quartz - Polycarbonate</p>"},{"location":"documentation/#sample-preparation","title":"\ud83e\uddf1 Sample Preparation","text":"<p>The sample used is a thin-section of blueschist, a metamorphic rock rich in: - Glaucophane - Zoisite - Garnet - Phengite</p> <p>Preparation Process: - Cut to 30\u202f\u00b5m thickness and mounted on a glass slide. - Mechanically polished using aluminum oxide. - Chemically polished using colloidal silica. - Grain Selection: 12 glaucophane grains selected based on crystallographic orientation via electron backscatter diffraction (EBSD).</p>"},{"location":"documentation/#experimental-parameters","title":"\u2699\ufe0f Experimental Parameters","text":""},{"location":"documentation/#tip-radius-load-settings","title":"Tip Radius &amp; Load Settings","text":"Tip Type Nominal Radius Actual Radius Peak Load 6 \u00b5m 6 \u00b5m 5.323 \u00b5m 200 mN"},{"location":"documentation/#loading-profiles","title":"Loading Profiles","text":"Type Ramp Duration Notes Slow Loading 60 seconds Gentler loading phase <p>All other parameters (e.g., hold, unload) were identical across experiments.</p>"},{"location":"documentation/#file-naming-convention","title":"\ud83d\udcc2 File Naming Convention","text":"<p>Each experiment file name consists of six components separated by underscores:</p> <ul> <li> <p>grain_.tdm <li> <p>Example: <code>grain5_6um_01_HL_QS_LC.tdm</code></p> </li>"},{"location":"documentation/#components","title":"Components","text":"Part Description <code>grain</code> Denotes experiment location or group <code>size</code> Tip radius (approximate; may update to actual radius in future) <code>name+number</code> Label for experiment within the grain group <code>HL</code> or <code>LL</code> Transducer: HL = High Load, LL = Low Load <code>QS</code> or <code>DMA</code> Mode: QS = Quasistatic, DMA = Continuous Property Measurement <code>LC/DC/OL</code> Load Control: LC = Load-Controlled, DC = Displacement-Controlled, OL = Open Loop <p>\ud83e\udde0 Note: While the last three identifiers are mostly used by the acquisition software, they may still hold value for sorting and statistical comparison. They're not essential for preprocessing but may help in interpreting mechanical response differences.</p>"},{"location":"documentation/#mapping-tip-load-speed-6-micron-tip","title":"\ud83d\uddc2 Mapping Tip &amp; Load Speed (6 Micron Tip)","text":"File Identifier Peak Load Loading Type 6 um 200 mN Slow* <p>Slow = 60s ramp \u2014 not always obvious from the file name alone; must be manually mapped.</p>"},{"location":"documentation/#experimental-images","title":"\ud83d\udcf8 Experimental Images","text":"<p>Below are a series of microstructural and equipment images related to the nanoindentation experiments.</p>"},{"location":"documentation/#electron-backscatter-diffraction-ebsd-maps","title":"\ud83d\udd2c Electron Backscatter Diffraction (EBSD) Maps","text":"<p>These EBSD maps show the microstructural grains in the blueschist rock sample.</p>"},{"location":"documentation/#1-inverse-pole-figure-ipf-coloring","title":"1. Inverse Pole Figure (IPF) Coloring","text":""},{"location":"documentation/#2-phase-map-with-mineral-segmentation","title":"2. Phase Map with Mineral Segmentation","text":""},{"location":"documentation/#grain-selection-for-indentation","title":"\ud83d\udccd Grain Selection for Indentation","text":"<p>The following image shows selected grains (e.g., Grain 4, 5, 7, 11, 12) that were targeted for indentation:</p> <p></p>"},{"location":"documentation/#instrumentation_1","title":"\ud83e\uddea Instrumentation","text":"<p>The nanoindentation experiments were conducted using the Bruker Hysitron TI 990 TriboIndenter.</p>"},{"location":"documentation/#1-close-up-of-the-tip-and-sample-stage","title":"1. Close-up of the Tip and Sample Stage","text":""},{"location":"documentation/#2-full-view-of-the-instrument","title":"2. Full View of the Instrument","text":""},{"location":"installation/","title":"Installation","text":"<p>Install from PyPI (once published):</p> <pre><code># From PyPI\npip install merrypopins\n\n# For development\ngit clone https://github.com/SerpRateAI/merrypopins.git\ncd merrypopins\npip install -e .\n</code></pre> <p>merrypopins supports Python 3.10+ and depends on:</p> <ul> <li><code>matplotlib</code></li> <li><code>numpy</code></li> <li><code>pandas</code></li> <li><code>scipy</code></li> <li><code>scikit-learn</code></li> <li><code>tensorflow</code></li> </ul> <p>These are installed automatically via <code>pip</code>.</p> <p>All core and development dependencies are tested with Python 3.10 through 3.12.</p>"},{"location":"installation/#development-testing","title":"Development &amp; Testing","text":"<ol> <li>Install development requirements:    <pre><code> # For development (includes dev tools like pytest, black, ruff, etc.)\n pip install -e '.[dev]'\n</code></pre>    This installs the main package and development dependencies listed in pyproject.toml under [project.optional-dependencies].dev</li> </ol> <p>Optionally, you can install development dependencies via:    <pre><code>pip install -r requirements-dev.txt\n</code></pre></p>"},{"location":"installation/#pre-commit-hooks","title":"\ud83d\udd27 Pre-commit Hooks","text":"<p>We rely on pre-commit to auto-run ruff (lint) and black (format) against every change before it is committed. If these checks are not executed locally, your PR will fail in CI.</p> <p>\ud83d\udea8 Important: You must have the <code>pre-commit</code> package installed globally (<code>pip install --user pre-commit</code> or via the project\u2019s dev extras) before making commits.</p>"},{"location":"installation/#setup-run-once-per-clone","title":"Setup (Run once per clone)","text":"<pre><code># 1) Install the tool (only needed if it\u2019s not already on your system)\npip install pre-commit          # or: pip install -e '.[dev]'\n\n# 2) Install the Git hooks defined in .pre-commit-config.yaml\npre-commit install\n</code></pre> <p>This adds a Git hook that formats / lints the staged files automatically at each <code>git commit</code>.</p> <p>Run Checks Manually</p> <p>To run all checks on all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"installation/#what-if-the-hook-rejects-my-commit","title":"What if the hook rejects my commit?","text":"<p>If <code>pre-commit</code> finds issues (usually formatting via black or lint via ruff), the commit will abort and the affected files will be modified in-place to satisfy the rules.</p> <ol> <li>Open Source Control (e.g. the Git sidebar in VS Code).  </li> <li>You will see the updated (but unstaged) files.</li> <li>Click the \u2795 (stage) button next to each fixed file or <code>git add &lt;file&gt;</code>.</li> <li>Re-run <code>git commit</code> \u2013 it should now succeed.</li> <li>Finally, push your branch to the remote.</li> </ol> <p>Tip: always run <code>pre-commit run --all-files</code> before making a commit to catch issues early.</p> <p>Notes: - Hooks are defined in <code>.pre-commit-config.yaml</code>. - You can exclude specific files or directories (e.g., <code>tutorials/</code>) by modifying the config file <code>.pre-commit-config.yaml</code>. - CI will re-run the same hooks; commits that bypass them locally will be rejected.</p>"},{"location":"installation/#running-tests","title":"\ud83e\uddea Running Tests","text":"<ol> <li> <p>Run tests with coverage:    <pre><code>pytest --cov=merrypopins --cov-report=term-missing\n</code></pre>    This command runs all tests in the <code>tests/</code> directory and generates a coverage report showing which lines of code were executed during the tests.    Tests and linting are automatically run on each pull request via GitHub Actions. The CI uses Python 3.10\u20133.12 and runs pre-commit, pytest, and coverage checks.</p> </li> <li> <p>Generate HTML coverage report:    <pre><code>pytest --cov=merrypopins --cov-report=html\n# open htmlcov/index.html in browser\n</code></pre></p> </li> </ol>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#importing-merrypopins-modules","title":"Importing merrypopins Modules","text":"<pre><code>from pathlib import Path\nfrom merrypopins.load_datasets import load_txt, load_tdm\nfrom merrypopins.preprocess import default_preprocess, remove_pre_min_load, rescale_data, finalise_contact_index\nfrom merrypopins.locate import default_locate\nfrom merrypopins.make_dataset import merrypopins_pipeline\nfrom merrypopins.statistics import default_statistics, calculate_stress_strain, calculate_stress_strain_statistics, default_statistics_stress_strain\n</code></pre>"},{"location":"quickstart/#load-indentation-data-and-metadata","title":"Load Indentation Data and Metadata","text":"<pre><code># 1) Load indentation data:\ndata_file = Path(\"data/experiment1.txt\")\ndf = load_txt(data_file)\nprint(df.head())\nprint(\"Timestamp:\", df.attrs['timestamp'])\nprint(\"Number of Points:\", df.attrs['num_points'])\n\n# 2) Load tdm metadata:\ntdm_meta_file = Path(\"data/experiment1.tdm\")\n# Load tdm metadata and channels this will create dataframe for root and channels\ndf_tdm_meta_root, df_tdm_meta_channels = load_tdm(tdm_meta_file)\n# The root metadata is stored as one row with their respective columns\nprint(df_tdm_meta_root.head())\n# To be able to read all the columns of root metadata dataframe it can be transposed\ndf_tdm_meta_root = df_tdm_meta_root.T.reset_index()\ndf_tdm_meta_root.columns = ['attribute', 'value']\nprint(df_tdm_meta_root.head(50))\n# The channel metadata is stored as multiple rows with their respective columns\nprint(df_tdm_meta_channels.head(50))\n</code></pre>"},{"location":"quickstart/#preprocess-data","title":"Preprocess Data","text":""},{"location":"quickstart/#option-1-use-default-pipeline","title":"Option 1: Use default pipeline","text":"<pre><code># This applies:\n# 1. Removes all rows before minimum Load\n# 2. Detects contact point and shifts Depth so contact = 0\n# 3. Removes Depth &lt; 0 rows and adds a flag for the contact point\n\ndf_processed = default_preprocess(df)\n\nprint(df_processed.head())\nprint(\"Contact point index:\", df_processed[df_processed[\"contact_point\"]].index[0])\n</code></pre>"},{"location":"quickstart/#option-2-customize-each-step-with-optional-arguments","title":"Option 2: Customize each step (with optional arguments)","text":"<p><pre><code># Step 1: Remove initial noise based on minimum Load\ndf_clean = remove_pre_min_load(df, load_col=\"Load (\u00b5N)\")\n\n# Step 2: Automatically detect contact point and zero the depth\ndf_rescaled = rescale_data(\n    df_clean,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n    N_baseline=30,     # number of points for baseline noise estimation\n    k=5.0,             # noise threshold multiplier\n    window_length=7,   # Savitzky-Golay smoothing window (must be odd)\n    polyorder=2        # Polynomial order for smoothing\n)\n\n# Step 3: Trim rows before contact and/or flag the point\ndf_final = finalise_contact_index(\n    df_rescaled,\n    depth_col=\"Depth (nm)\",\n    remove_pre_contact=True,       # remove rows where depth &lt; 0\n    add_flag_column=True,          # add a boolean column marking the contact point\n    flag_column=\"contact_point\"    # customize the column name if needed\n)\n\nprint(df_final[df_final[\"contact_point\"]])  # display contact row\nprint(\"Contact point index:\", df_final[df_final[\"contact_point\"]].index[0])\n</code></pre> \ud83e\uddea Tip You can omit or modify any step depending on your data:</p> <ul> <li>Skip remove_pre_min_load() if your data is already clean.</li> <li>Set remove_pre_contact=False if you want to retain all data.</li> <li>Customize flag_column to integrate with your own schema.</li> </ul>"},{"location":"quickstart/#locate-pop-in-events","title":"Locate Pop-in Events","text":""},{"location":"quickstart/#detect-pop-ins-using-default-method","title":"Detect Pop-ins using Default Method","text":"<pre><code># Detect pop-ins using all methods\nresults = default_locate(df_processed)\nprint(results[results.popin])\n</code></pre>"},{"location":"quickstart/#customize-detection-thresholds","title":"Customize Detection Thresholds","text":"<pre><code>results_tuned = default_locate(\n    df_processed,\n    iforest_contamination=0.002,\n    cnn_threshold_multiplier=4.0,\n    fd_threshold=2.5,\n    savgol_threshold=2.0\n)\n</code></pre>"},{"location":"quickstart/#visualize-detections","title":"Visualize Detections","text":"<pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,6))\nplt.plot(results_tuned[\"Depth (nm)\"], results_tuned[\"Load (\u00b5N)\"], label=\"Preprocessed\", alpha=0.4, color='orange')\n\ncolors = {\n    \"popin_iforest\": 'red',\n    \"popin_cnn\": 'purple',\n    \"popin_fd\": 'darkorange',\n    \"popin_savgol\": 'green'\n}\nmarkers = {\n    \"popin_iforest\": '^',\n    \"popin_cnn\": 'v',\n    \"popin_fd\": 'x',\n    \"popin_savgol\": 'D'\n}\n\nfor method, color in colors.items():\n    mdf = results_tuned[results_tuned[method]]\n    plt.scatter(mdf[\"Depth (nm)\"], mdf[\"Load (\u00b5N)\"],\n                c=color, label=method.replace(\"popin_\", \"\").capitalize(),\n                marker=markers[method], alpha=0.7)\n\nconfident = results_tuned[results_tuned[\"popin_confident\"]]\nplt.scatter(confident[\"Depth (nm)\"], confident[\"Load (\u00b5N)\"],\n            edgecolors='k', facecolors='none', label=\"Majority Vote (2+)\", s=100, linewidths=1.5)\n\nplt.xlabel(\"Depth (nm)\"); plt.ylabel(\"Load (\u00b5N)\")\nplt.title(\"Pop-in Detections by All Methods\")\nplt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n</code></pre>"},{"location":"quickstart/#run-full-pipeline-with-merrypopins_pipeline","title":"Run Full Pipeline with merrypopins_pipeline","text":"<p>This function runs the entire merrypopins workflow, from loading data to locating pop-ins and generating visualizations.</p>"},{"location":"quickstart/#define-input-and-output-paths","title":"Define Input and Output Paths","text":"<pre><code># Define the text file that will be processed and output directory that will contain the visualization\ntext_file = Path(\"datasets/6microntip_slowloading/grain9_6um_indent03_HL_QS_LC.txt\")\noutput_dir = Path(\"visualisations/6microntip_slowloading/grain9_6um_indent03_HL_QS_LC\")\n\n# Make sure output directory exists\noutput_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"quickstart/#run-the-merrypopins-pipeline","title":"Run The merrypopins Pipeline","text":"<pre><code>df_pipeline = merrypopins_pipeline(\n    text_file,\n    save_plot_dir=output_dir,\n    trim_margin=30\n)\n</code></pre>"},{"location":"quickstart/#view-result-dataframe","title":"View Result DataFrame","text":"<pre><code>df_pipeline.head()\n</code></pre>"},{"location":"quickstart/#view-result-visualizations","title":"View Result Visualizations","text":"<pre><code># The pipeline generates plot in the specified output directory for the provided text file.\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Load all PNGs from output folder\nimage_paths = sorted(output_dir.glob(\"*.png\"))\n\n# Only proceed if there are images\nif image_paths:\n    img = Image.open(image_paths[0])\n    plt.figure(figsize=(12, 6))\n    plt.imshow(img)\n    plt.title(image_paths[0].stem)\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"No plots found in output folder.\")\n</code></pre>"},{"location":"quickstart/#calculate-pop-in-statistics","title":"Calculate Pop-in Statistics","text":""},{"location":"quickstart/#calculate-pop-in-statistics-load-depth","title":"Calculate Pop-in Statistics (Load-Depth)","text":"<pre><code>df_statistics = default_statistics(df_pipeline)\n\n# View the computed statistics for each pop-in\nprint(df_statistics.head())\n</code></pre>"},{"location":"quickstart/#calculate-stress-strain-statistics","title":"Calculate Stress-Strain Statistics","text":""},{"location":"quickstart/#perform-stress-strain-transformation-and-statistics","title":"Perform Stress-Strain Transformation and Statistics","text":"<pre><code># Perform stress-strain transformation\ndf_stress_strain = calculate_stress_strain(df_statistics)\n\n# Calculate stress-strain statistics\ndf_stress_strain_statistics = calculate_stress_strain_statistics(df_stress_strain)\n\n# View the calculated stress-strain statistics\nprint(df_stress_strain_statistics.head())\n</code></pre>"},{"location":"quickstart/#full-statistics-pipeline","title":"Full Statistics Pipeline","text":""},{"location":"quickstart/#perform-default-full-statistics-pipeline-for-stress-strain","title":"Perform Default Full Statistics Pipeline for Stress-Strain","text":"<pre><code>df_statistics_stress_strain = default_statistics_stress_strain(\n    df_pipeline,\n    popin_flag_column=\"popin\",\n    before_window=0.5,\n    after_window=0.5,\n    Reff_um=5.323,\n    min_load_uN=2000,\n    smooth_stress=True,\n    stress_col=\"stress\",\n    strain_col=\"strain\",\n    time_col=\"Time (s)\",\n)\n\n# View the final stress-strain statistics\nprint(df_statistics_stress_strain.head())\n</code></pre>"},{"location":"streamlit/","title":"Merrypopins Streamlit App","text":""},{"location":"streamlit/#run-merrypopins-streamlit-app","title":"\ud83d\udce6 Run Merrypopins Streamlit App","text":"<p>Merrypopins includes an interactive Streamlit app for visualizing and detecting pop-ins in indentation data. This app allows you to upload your data files, run the detection algorithms, and visualize the results in a user-friendly interface.</p>"},{"location":"streamlit/#try-it-online","title":"\ud83c\udf10 Try It Online","text":"<p>\ud83d\ude80 Live demo: explore Merrypopins in your browser! </p>"},{"location":"streamlit/#using-docker","title":"\ud83d\udc33 Using Docker","text":"<p>You can run the interactive Streamlit app for visualizing and detecting pop-ins directly using Docker.</p>"},{"location":"streamlit/#option-1-build-and-run-locally","title":"\ud83d\udd27 Option 1: Build and Run Locally","text":"<pre><code># Clone the repo if not already\ngit clone https://github.com/SerpRateAI/merrypopins.git\ncd merrypopins\n\n# Build the Docker image\ndocker build -t merrypopins-app .\n\n# Run the app on http://localhost:8501\ndocker run -p 8501:8501 merrypopins-app\n</code></pre>"},{"location":"streamlit/#option-2-pull-and-run-pre-built-image-from-docker-hub-recommended-takes-less-time","title":"\ud83c\udf10 Option 2: Pull and Run Pre-built Image from Docker Hub (Recommended Takes Less Time)","text":"<pre><code># Pull the latest pre-built image from Docker Hub\ndocker pull cacarvuai/merrypopins-app:latest\n\n# Run the container\ndocker run -p 8501:8501 cacarvuai/merrypopins-app:latest\n</code></pre>"},{"location":"streamlit/#access-the-app","title":"\ud83c\udf1f Access the App","text":"<p>Once the app is running, you can access it in your web browser at http://localhost:8501.</p>"},{"location":"streamlit/#clean-up","title":"\ud83e\uddfc Clean Up","text":"<p>To stop the app, press <code>Ctrl+C</code> (on Windows/Linux) or <code>\u2318(Command)+C</code> (on MacOS) in the terminal where it's running.</p> <p>If you want to remove the Docker container, you can run:</p> <pre><code>docker rm -f $(docker ps -aq --filter \"ancestor=cacarvuai/merrypopins-app:latest\")\n</code></pre> <p>If you built the image locally, you can remove it with:</p> <pre><code>docker rmi merrypopins-app\n</code></pre>"},{"location":"streamlit/#running-the-app-locally-without-docker","title":"Running the App Locally Without Docker","text":"<p>If you prefer to run the Streamlit app without Docker, you can do so by following these steps:</p> <ol> <li> <p>Install the required dependencies for the app:    <pre><code>pip install -r streamlit_app/requirements.txt\n</code></pre></p> </li> <li> <p>Run the Streamlit app:    <pre><code>streamlit run streamlit_app/app.py\n</code></pre></p> </li> <li> <p>Open your web browser and go to http://localhost:8501 to access the app.</p> </li> </ol>"},{"location":"streamlit/#publishing-new-app-versions-to-docker-hub-only-for-maintainers","title":"Publishing New App Versions to Docker Hub (Only for Maintainers)","text":"<p>To publish a new version of the Merrypopins Streamlit app, follow these steps:</p> <ol> <li>Update the version date in <code>streamlit_app/app.py</code>.</li> <li>Commit your changes to the Git repository.</li> <li>Push the changes to the remote repository.</li> <li>Build and push the updated Docker image to Docker Hub:    <pre><code>docker login\ndocker buildx build \\\n --platform linux/amd64,linux/arm64 \\\n -t cacarvuai/merrypopins-app:latest \\\n --push .\n</code></pre></li> <li>Update the documentation to reflect the new version.</li> </ol> <p>Note: If you are not a maintainer, please do not attempt to publish new versions. Instead, you can contribute by submitting issues or pull requests with improvements or bug fixes. If you want to create your own version of the app, you can fork the repository and modify the code as needed. Just remember to change the Docker image name to avoid conflicts with the official version. e.g <code>docker build -t yourdockerhubusername/merrypopins-app:latest .</code></p>"},{"location":"reference/merrypopins.load_datasets/","title":"load_datasets","text":""},{"location":"reference/merrypopins.load_datasets/#merrypopins.load_datasets--load_datasetspy","title":"load_datasets.py","text":"<p>Read indentation experiment TXT data and metadata files into pandas DataFrames. Provides:   - load_txt: load a .txt data file (auto-detect columns) into a DataFrame, with header attrs   - load_tdm: load a .tdm/.tdx metadata file (full channel list) into two DataFrames (root info and channel list)</p> Usage <p>from merrypopins.load_datasets import load_txt, load_tdm</p>"},{"location":"reference/merrypopins.load_datasets/#merrypopins.load_datasets.load_tdm","title":"<code>load_tdm(filepath)</code>","text":"<p>Load a .tdm metadata file into two DataFrames. Args:     filepath: Path to the .tdm/.tdx file. Returns:   - df_root: one row containing       * name, description, title, author       * every instance\u2010attribute under    - df_channels: one row per  with:       group, channel_id, name, unit, description, datatype,       sequence_id, block_id, block_length, value_type Raises:     FileNotFoundError: If the file does not exist. Source code in <code>src/merrypopins/load_datasets.py</code> <pre><code>def load_tdm(filepath: Path):\n    \"\"\"\n    Load a .tdm metadata file into two DataFrames.\n    Args:\n        filepath: Path to the .tdm/.tdx file.\n    Returns:\n      - df_root: one row containing\n          * name, description, title, author\n          * every instance\u2010attribute under &lt;instance_attributes&gt;\n      - df_channels: one row per &lt;tdm_channel&gt; with:\n          group, channel_id, name, unit, description, datatype,\n          sequence_id, block_id, block_length, value_type\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    if not filepath.is_file():\n        raise FileNotFoundError(f\"TDM file not found: {filepath}\")\n    tree = ET.parse(str(filepath))\n    root = tree.getroot()\n\n    # --- extract tdm_root info ---\n    tr = root.find(\".//tdm_root\")\n    root_info = {\n        \"root_name\": tr.findtext(\"name\"),\n        \"root_description\": tr.findtext(\"description\"),\n        \"root_title\": tr.findtext(\"title\"),\n        \"root_author\": tr.findtext(\"author\"),\n    }\n    inst = tr.find(\"instance_attributes\")\n    for attr in inst:\n        # double_attribute, string_attribute, long_attribute, time_attribute...\n        key = attr.get(\"name\")\n        # string_attribute has &lt;s&gt; contents, others have .text\n        if attr.tag.endswith(\"string_attribute\"):\n            val = attr.findtext(\"s\")\n        else:\n            val = attr.text\n        # strip leading/trailing whitespace (incl. newlines and tabs)\n        val = val.strip() if isinstance(val, str) else val\n        root_info[key] = val\n    df_root = pd.DataFrame([root_info])\n\n    # --- build helper maps for channels ---\n    # 1) group id \u2192 group name\n    group_map = {\n        g.get(\"id\"): g.findtext(\"name\") for g in root.findall(\".//tdm_channelgroup\")\n    }\n\n    # 2) channel \u2192 sequence via localcolumn\n    chan2seq = {}\n    for lc in root.findall(\".//localcolumn\"):\n        m1 = re.search(r'id\\(\"([^\"]+)\"\\)', (lc.findtext(\"measurement_quantity\") or \"\"))\n        m2 = re.search(r'id\\(\"([^\"]+)\"\\)', (lc.findtext(\"values\") or \"\"))\n        if m1 and m2:\n            chan2seq[m1.group(1)] = m2.group(1)\n\n    # --- now build per\u2010channel rows ---\n    records = []\n    for c in root.findall(\".//tdm_channel\"):\n        cid = c.get(\"id\")\n        grp_txt = c.findtext(\"group\") or \"\"\n        m = re.search(r'id\\(\"([^\"]+)\"\\)', grp_txt)\n        group = group_map.get(m.group(1)) if m else None\n\n        seq = chan2seq.get(cid)\n        # blk = seq2blk.get(seq)  # unused \u2192 removed\n        rec = {\n            \"group\": group,\n            \"channel_id\": cid,\n            \"name\": c.findtext(\"name\"),\n            \"unit\": c.findtext(\"unit_string\"),\n            \"description\": c.findtext(\"description\"),\n            \"datatype\": c.findtext(\"datatype\"),\n            \"sequence_id\": seq,\n        }\n        records.append(rec)\n\n    df_channels = pd.DataFrame.from_records(records)\n    logger.info(f\"Loaded TDM metadata {filepath.name}: {len(df_channels)} channels\")\n\n    return df_root, df_channels\n</code></pre>"},{"location":"reference/merrypopins.load_datasets/#merrypopins.load_datasets.load_txt","title":"<code>load_txt(filepath)</code>","text":"<p>Load a .txt indentation data file into a DataFrame. Automatically detects the header line (column names) and numeric block.</p> <p>Attempts UTF-8 decoding first, falls back to Latin-1 on failure.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path</code> <p>Path to the .txt file.</p> required <p>Returns:     DataFrame with columns from the file, and attrs:       - timestamp: first non-empty line       - num_points: parsed from 'Number of Points = N'       - Depth (nm): parsed from the first column with the name \"Depth (nm)\"       - Load (\u00b5N): parsed from the second column with the name \"Load (uN)\"       - Time (s): parsed from the third column with the name \"Time (s)\" Raises:     FileNotFoundError: If the file does not exist.     NotImplementedError: If the file type is not supported.     UnicodeDecodeError: If the file cannot be decoded with UTF-8 or Latin-1.     ValueError: If no numeric data is found in the file.</p> Source code in <code>src/merrypopins/load_datasets.py</code> <pre><code>def load_txt(filepath: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a .txt indentation data file into a DataFrame.\n    Automatically detects the header line (column names) and numeric block.\n\n    Attempts UTF-8 decoding first, falls back to Latin-1 on failure.\n\n    Args:\n        filepath: Path to the .txt file.\n    Returns:\n        DataFrame with columns from the file, and attrs:\n          - timestamp: first non-empty line\n          - num_points: parsed from 'Number of Points = N'\n          - Depth (nm): parsed from the first column with the name \"Depth (nm)\"\n          - Load (\u00b5N): parsed from the second column with the name \"Load (uN)\"\n          - Time (s): parsed from the third column with the name \"Time (s)\"\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        NotImplementedError: If the file type is not supported.\n        UnicodeDecodeError: If the file cannot be decoded with UTF-8 or Latin-1.\n        ValueError: If no numeric data is found in the file.\n    \"\"\"\n    # Check if the file exists\n    if not filepath.is_file():\n        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n\n    # Check file extension if it is a .txt file if not raise NotImplementedError\n    if filepath.suffix.lower() != \".txt\":\n        raise NotImplementedError(\n            f\"File type '{filepath.suffix}' is not supported yet. Only '.txt' files are currently implemented.\"\n        )\n\n    # Read lines with encoding fallback\n    try:\n        raw = filepath.read_text(encoding=\"utf-8\")\n    except UnicodeDecodeError:\n        logger.warning(f\"UTF-8 decode failed for {filepath}, falling back to Latin-1\")\n        try:\n            raw = filepath.read_text(encoding=\"latin1\")\n        except Exception as e:\n            logger.error(f\"Latin-1 decode also failed for {filepath}.\")\n            raise e\n    text = raw.splitlines()\n\n    # Extract timestamp and num_points\n    timestamp = None\n    num_points = None\n    for line in text:\n        if timestamp is None and line.strip():\n            timestamp = line.strip()\n        if \"Number of Points\" in line and \"=\" in line:\n            try:\n                num_points = int(line.split(\"=\", 1)[1])\n            except ValueError:\n                pass\n        if timestamp and num_points is not None:\n            break\n\n    # Find start of numeric block: first row where every tab-split token is a number\n    start_idx = None\n    num_re = re.compile(r\"^[-+]?\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?$\")\n    for i, line in enumerate(text):\n        tokens = line.strip().split(\"\\t\")\n        if tokens and all(num_re.match(tok) for tok in tokens):\n            start_idx = i\n            break\n    if start_idx is None:\n        raise ValueError(f\"No numeric data found in {filepath}\")\n\n    # Header is the last non-empty line before the numeric block\n    header_idx = start_idx - 1\n    while header_idx &gt;= 0 and not text[header_idx].strip():\n        header_idx -= 1\n    if header_idx &gt;= 0:\n        col_names = text[header_idx].split(\"\\t\")\n    else:\n        col_names = []\n\n    # Load the numeric block with tab delimiter\n    data_str = \"\\n\".join(text[start_idx:])\n    arr = np.loadtxt(StringIO(data_str), delimiter=\"\\t\")\n\n    # Force 2D array\n    if arr.ndim == 0:\n        arr = arr.reshape(1, 1)\n    elif arr.ndim == 1:\n        arr = arr.reshape(-1, 1)\n\n    # If header didn\u2019t match, generate generic names\n    if not col_names or len(col_names) != arr.shape[1]:\n        col_names = [f\"col_{i}\" for i in range(arr.shape[1])]\n\n    df = pd.DataFrame(arr, columns=col_names)\n    df.attrs[\"timestamp\"] = timestamp\n    df.attrs[\"num_points\"] = num_points\n    logger.info(f\"Loaded TXT data {filepath.name}: {df.shape[0]}\u2009\u00d7\u2009{df.shape[1]}\")\n    return df\n</code></pre>"},{"location":"reference/merrypopins.locate/","title":"locate","text":""},{"location":"reference/merrypopins.locate/#merrypopins.locate--locatepy","title":"locate.py","text":"<p>Detects pop-ins (sudden displacement jumps) in nano-indentation curves using multiple methods:</p> <p>\u2022 IsolationForest anomaly detection on stiffness and curvature features \u2022 CNN-based autoencoder reconstruction error \u2022 Finite difference method using Fourier spectral analysis \u2022 Savitzky-Golay derivative method</p> <p>To ensure relevance, all detection methods operate only on the indentation curve up to the maximum load point. This is because pop-in events occur during the loading phase of indentation. After reaching peak load, material unloading or post-penetration artifacts may dominate, which are irrelevant for pop-in analysis.</p> <p>Provides: - compute_stiffness - compute_features - detect_popins_iforest - detect_popins_cnn - detect_popins_fd_fourier - detect_popins_savgol - default_locate (combines all methods)</p>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.build_cnn_autoencoder","title":"<code>build_cnn_autoencoder(window_size, n_features)</code>","text":"<p>Build a 1D Convolutional Autoencoder for time-series anomaly detection.</p> <p>This model learns to reconstruct input sequences composed of features like stiffness difference and curvature. During inference, reconstruction error is used to detect anomalies\u2014samples with high error are likely pop-ins.</p> Architecture overview <ul> <li>Encoder:     Conv1D -&gt; MaxPooling -&gt; Conv1D -&gt; MaxPooling -&gt; Conv1D</li> <li>Decoder:     UpSampling -&gt; Conv1D -&gt; UpSampling -&gt; Conv1D</li> </ul> <p>The model operates on fixed-size input windows and uses symmetric encoding and decoding layers. The final layer has linear activation to match the original feature values.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Number of time steps per sequence.</p> required <code>n_features</code> <code>int</code> <p>Number of input features per time step.</p> required <p>Returns:</p> Type Description <p>keras.Model: Keras autoencoder model (uncompiled).</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def build_cnn_autoencoder(window_size, n_features):\n    \"\"\"\n    Build a 1D Convolutional Autoencoder for time-series anomaly detection.\n\n    This model learns to reconstruct input sequences composed of features like\n    stiffness difference and curvature. During inference, reconstruction error\n    is used to detect anomalies\u2014samples with high error are likely pop-ins.\n\n    Architecture overview:\n      - Encoder:\n          Conv1D -&gt; MaxPooling -&gt; Conv1D -&gt; MaxPooling -&gt; Conv1D\n      - Decoder:\n          UpSampling -&gt; Conv1D -&gt; UpSampling -&gt; Conv1D\n\n    The model operates on fixed-size input windows and uses symmetric encoding\n    and decoding layers. The final layer has linear activation to match the\n    original feature values.\n\n    Args:\n        window_size (int): Number of time steps per sequence.\n        n_features (int): Number of input features per time step.\n\n    Returns:\n        keras.Model: Keras autoencoder model (uncompiled).\n    \"\"\"\n    inp = Input(shape=(window_size, n_features))\n    x = Conv1D(32, 3, activation=\"relu\", padding=\"same\")(inp)\n    x = MaxPooling1D(2, padding=\"same\")(x)\n    x = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(x)\n    x = MaxPooling1D(2, padding=\"same\")(x)\n    x = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(x)\n    x = UpSampling1D(2)(x)\n    x = Conv1D(32, 3, activation=\"relu\", padding=\"same\")(x)\n    x = UpSampling1D(2)(x)\n    x = Conv1D(n_features, 3, activation=\"linear\", padding=\"same\")(x)\n    return Model(inp, x)\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.compute_features","title":"<code>compute_features(df, depth_col='Depth (nm)', load_col='Load (\u00b5N)', window=5, return_derivatives=True)</code>","text":"<p>Compute derived indentation features for anomaly detection.</p> This function calculates three features <ol> <li>Stiffness: local slope of load vs. depth (\u0394Load/\u0394Depth)</li> <li>Stiffness difference: the rate of change in stiffness (first derivative)</li> <li>Curvature: the rate of change in stiffness difference (second derivative)</li> </ol> <p>These features help detect sudden shifts in indentation behavior, often indicative of pop-in events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data.</p> required <code>depth_col</code> <code>str</code> <p>Column name for depth.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Column name for load.</p> <code>'Load (\u00b5N)'</code> <code>window</code> <code>int</code> <p>Sliding window size for stiffness calculation.</p> <code>5</code> <code>return_derivatives</code> <code>bool</code> <p>If True (default), return DataFrame with added features.                        If False, return original DataFrame without added columns.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Enhanced DataFrame with 'stiffness', 'stiff_diff', and 'curvature' columns.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def compute_features(\n    df, depth_col=\"Depth (nm)\", load_col=\"Load (\u00b5N)\", window=5, return_derivatives=True\n):\n    \"\"\"\n    Compute derived indentation features for anomaly detection.\n\n    This function calculates three features:\n      1. Stiffness: local slope of load vs. depth (\u0394Load/\u0394Depth)\n      2. Stiffness difference: the rate of change in stiffness (first derivative)\n      3. Curvature: the rate of change in stiffness difference (second derivative)\n\n    These features help detect sudden shifts in indentation behavior, often indicative\n    of pop-in events.\n\n    Args:\n        df (DataFrame): Input indentation data.\n        depth_col (str): Column name for depth.\n        load_col (str): Column name for load.\n        window (int): Sliding window size for stiffness calculation.\n        return_derivatives (bool): If True (default), return DataFrame with added features.\n                                   If False, return original DataFrame without added columns.\n\n    Returns:\n        DataFrame: Enhanced DataFrame with 'stiffness', 'stiff_diff', and 'curvature' columns.\n    \"\"\"\n    df2 = df.copy()\n    df2[\"stiffness\"] = compute_stiffness(df, depth_col, load_col, window)\n    df2[\"stiff_diff\"] = df2[\"stiffness\"].diff()\n    df2[\"curvature\"] = df2[\"stiff_diff\"].diff()\n    return df2 if return_derivatives else df\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.compute_stiffness","title":"<code>compute_stiffness(df, depth_col='Depth (nm)', load_col='Load (\u00b5N)', window=5)</code>","text":"<p>Compute local stiffness (dLoad/dDepth) using sliding-window linear regression.</p> <p>In nano-indentation, 'stiffness' is the local slope of the load\u2013depth curve and reflects how resistant the material is to deformation. It is computed as:</p> <pre><code>stiffness = change in Load / change in Depth\n          = \u0394Load / \u0394Depth\n</code></pre> <p>This is estimated using linear regression over a moving window centered on each point.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data.</p> required <code>depth_col</code> <code>str</code> <p>Column name for depth.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Column name for load.</p> <code>'Load (\u00b5N)'</code> <code>window</code> <code>int</code> <p>Sliding window size.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>Series</code> <p>Stiffness at each data point.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def compute_stiffness(df, depth_col=\"Depth (nm)\", load_col=\"Load (\u00b5N)\", window=5):\n    \"\"\"\n    Compute local stiffness (dLoad/dDepth) using sliding-window linear regression.\n\n    In nano-indentation, 'stiffness' is the local slope of the load\u2013depth curve and\n    reflects how resistant the material is to deformation. It is computed as:\n\n        stiffness = change in Load / change in Depth\n                  = \u0394Load / \u0394Depth\n\n    This is estimated using linear regression over a moving window centered on each point.\n\n    Args:\n        df (DataFrame): Input indentation data.\n        depth_col (str): Column name for depth.\n        load_col (str): Column name for load.\n        window (int): Sliding window size.\n\n    Returns:\n        Series: Stiffness at each data point.\n    \"\"\"\n    if depth_col not in df.columns or load_col not in df.columns:\n        raise ValueError(\n            f\"Required columns '{depth_col}' and/or '{load_col}' not found in DataFrame.\"\n        )\n    if len(df) &lt; window:\n        raise ValueError(\n            f\"Not enough data points ({len(df)}) for window size ({window}).\"\n        )\n\n    x, y = df[depth_col].values, df[load_col].values\n    stiffness = np.full(len(x), np.nan)\n    half_win = window // 2\n\n    for i in range(half_win, len(x) - half_win):\n        dx = x[i - half_win : i + half_win + 1]\n        dy = y[i - half_win : i + half_win + 1]\n        A = np.vstack([dx, np.ones_like(dx)]).T\n        stiffness[i], _ = np.linalg.lstsq(A, dy, rcond=None)[0]\n\n    return pd.Series(stiffness, index=df.index)\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.default_locate","title":"<code>default_locate(df, iforest_contamination=0.001, iforest_random_state=None, cnn_window_size=64, cnn_epochs=10, cnn_threshold_multiplier=5.0, cnn_batch_size=32, cnn_validation_split=0.0, fd_threshold=3.0, fd_spacing=1.0, savgol_window_length=11, savgol_polyorder=2, savgol_threshold=3.0, sg_deriv_order=1, stiffness_window=5, trim_edges_enabled=True, trim_margin=None, max_load_trim_enabled=True, use_iforest=True, use_cnn=True, use_fd=True, use_savgol=True, depth_col='Depth (nm)', load_col='Load (\u00b5N)')</code>","text":"<p>Apply all (default) or selected detection methods to identify pop-ins.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data.</p> required <code>iforest_contamination</code> <code>float</code> <p>Expected contamination level for IsolationForest.</p> <code>0.001</code> <code>iforest_random_state</code> <code>int or None</code> <p>Seed for reproducibility.</p> <code>None</code> <code>cnn_window_size</code> <code>int</code> <p>Window size for CNN autoencoder.</p> <code>64</code> <code>cnn_epochs</code> <code>int</code> <p>Training epochs for CNN.</p> <code>10</code> <code>cnn_threshold_multiplier</code> <code>float</code> <p>Threshold multiplier for CNN anomaly detection.</p> <code>5.0</code> <code>cnn_batch_size</code> <code>int</code> <p>Batch size for CNN autoencoder.</p> <code>32</code> <code>cnn_validation_split</code> <code>float</code> <p>Validation split for CNN autoencoder.</p> <code>0.0</code> <code>fd_threshold</code> <code>float</code> <p>Standard deviation threshold for finite difference method.</p> <code>3.0</code> <code>fd_spacing</code> <code>float</code> <p>Spacing between samples for FFT derivative.</p> <code>1.0</code> <code>savgol_window_length</code> <code>int</code> <p>Window size for Savitzky-Golay filter.</p> <code>11</code> <code>savgol_polyorder</code> <code>int</code> <p>Polynomial order for Savitzky-Golay filter.</p> <code>2</code> <code>savgol_threshold</code> <code>float</code> <p>Std deviation threshold for Savitzky-Golay.</p> <code>3.0</code> <code>sg_deriv_order</code> <code>int</code> <p>Derivative order for Savitzky-Golay.</p> <code>1</code> <code>stiffness_window</code> <code>int</code> <p>Sliding window size for stiffness computation.</p> <code>5</code> <code>trim_edges_enabled</code> <code>bool</code> <p>If True, trims the first <code>window</code> elements</p> <code>True</code> <code>trim_margin</code> <code>int or None</code> <p>Number of elements to trim from the start.</p> <code>None</code> <code>max_load_trim_enabled</code> <code>bool</code> <p>If True, masks out any anomalies after the maximum load point. Default is True.</p> <code>True</code> <code>use_iforest</code> <code>bool</code> <p>Whether to use IsolationForest method.</p> <code>True</code> <code>use_cnn</code> <code>bool</code> <p>Whether to use CNN method.</p> <code>True</code> <code>use_fd</code> <code>bool</code> <p>Whether to use finite difference method.</p> <code>True</code> <code>use_savgol</code> <code>bool</code> <p>Whether to use Savitzky-Golay method.</p> <code>True</code> <code>depth_col</code> <code>str</code> <p>Column name for depth data.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Column name for load data.</p> <code>'Load (\u00b5N)'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Data with individual method flags, combined flag, and metadata columns.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def default_locate(\n    df,\n    iforest_contamination=0.001,\n    iforest_random_state=None,\n    cnn_window_size=64,\n    cnn_epochs=10,\n    cnn_threshold_multiplier=5.0,\n    cnn_batch_size=32,\n    cnn_validation_split=0.0,\n    fd_threshold=3.0,\n    fd_spacing=1.0,\n    savgol_window_length=11,\n    savgol_polyorder=2,\n    savgol_threshold=3.0,\n    sg_deriv_order=1,\n    stiffness_window=5,\n    trim_edges_enabled=True,\n    trim_margin=None,\n    max_load_trim_enabled=True,\n    use_iforest=True,\n    use_cnn=True,\n    use_fd=True,\n    use_savgol=True,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n):\n    \"\"\"\n    Apply all (default) or selected detection methods to identify pop-ins.\n\n    Args:\n        df (DataFrame): Input indentation data.\n        iforest_contamination (float): Expected contamination level for IsolationForest.\n        iforest_random_state (int or None): Seed for reproducibility.\n        cnn_window_size (int): Window size for CNN autoencoder.\n        cnn_epochs (int): Training epochs for CNN.\n        cnn_threshold_multiplier (float): Threshold multiplier for CNN anomaly detection.\n        cnn_batch_size (int): Batch size for CNN autoencoder.\n        cnn_validation_split (float): Validation split for CNN autoencoder.\n        fd_threshold (float): Standard deviation threshold for finite difference method.\n        fd_spacing (float): Spacing between samples for FFT derivative.\n        savgol_window_length (int): Window size for Savitzky-Golay filter.\n        savgol_polyorder (int): Polynomial order for Savitzky-Golay filter.\n        savgol_threshold (float): Std deviation threshold for Savitzky-Golay.\n        sg_deriv_order (int): Derivative order for Savitzky-Golay.\n        stiffness_window (int): Sliding window size for stiffness computation.\n        trim_edges_enabled (bool): If True, trims the first `window` elements\n        trim_margin (int or None): Number of elements to trim from the start.\n        max_load_trim_enabled (bool): If True, masks out any anomalies after the maximum load point. Default is True.\n        use_iforest (bool): Whether to use IsolationForest method.\n        use_cnn (bool): Whether to use CNN method.\n        use_fd (bool): Whether to use finite difference method.\n        use_savgol (bool): Whether to use Savitzky-Golay method.\n        depth_col (str): Column name for depth data.\n        load_col (str): Column name for load data.\n\n    Returns:\n        DataFrame: Data with individual method flags, combined flag, and metadata columns.\n    \"\"\"\n    df_combined = df.copy()\n    method_flags = []\n\n    if use_iforest:\n        df_iforest = detect_popins_iforest(\n            df,\n            contamination=iforest_contamination,\n            random_state=iforest_random_state,\n            depth_col=depth_col,\n            load_col=load_col,\n            window=stiffness_window,\n            trim_edges_enabled=trim_edges_enabled,\n            trim_margin=trim_margin,\n            max_load_trim_enabled=max_load_trim_enabled,\n        )\n        df_combined[\"popin_iforest\"] = df_iforest[\"popin_iforest\"]\n        method_flags.append(\"popin_iforest\")\n\n    if use_cnn:\n        df_cnn = detect_popins_cnn(\n            df,\n            window_size=cnn_window_size,\n            epochs=cnn_epochs,\n            threshold_multiplier=cnn_threshold_multiplier,\n            batch_size=cnn_batch_size,\n            validation_split=cnn_validation_split,\n            depth_col=depth_col,\n            load_col=load_col,\n            window=stiffness_window,\n            trim_edges_enabled=trim_edges_enabled,\n            trim_margin=trim_margin,\n            max_load_trim_enabled=max_load_trim_enabled,\n        )\n        df_combined[\"popin_cnn\"] = df_cnn[\"popin_cnn\"]\n        method_flags.append(\"popin_cnn\")\n\n    if use_fd:\n        df_fd = detect_popins_fd_fourier(\n            df,\n            threshold=fd_threshold,\n            spacing=fd_spacing,\n            trim_edges_enabled=trim_edges_enabled,\n            trim_margin=trim_margin,\n            load_col=load_col,\n            max_load_trim_enabled=max_load_trim_enabled,\n        )\n        df_combined[\"popin_fd\"] = df_fd[\"popin_fd\"]\n        method_flags.append(\"popin_fd\")\n\n    if use_savgol:\n        df_savgol = detect_popins_savgol(\n            df,\n            window_length=savgol_window_length,\n            polyorder=savgol_polyorder,\n            threshold=savgol_threshold,\n            deriv=sg_deriv_order,\n            load_col=load_col,\n            trim_edges_enabled=trim_edges_enabled,\n            trim_margin=trim_margin,\n            max_load_trim_enabled=max_load_trim_enabled,\n        )\n        df_combined[\"popin_savgol\"] = df_savgol[\"popin_savgol\"]\n        method_flags.append(\"popin_savgol\")\n\n    df_combined[\"popin\"] = df_combined[method_flags].any(axis=1)\n    df_combined[\"popin_methods\"] = df_combined[method_flags].apply(\n        lambda row: \",\".join(\n            [col.replace(\"popin_\", \"\") for col in method_flags if row[col]]\n        ),\n        axis=1,\n    )\n    df_combined[\"popin_score\"] = df_combined[method_flags].sum(axis=1)\n    df_combined[\"popin_confident\"] = df_combined[\"popin_score\"] &gt;= 2\n\n    total_popins = df_combined[\"popin\"].sum()\n    logger.info(f\"Total pop-ins detected by selected methods: {total_popins}\")\n\n    return df_combined\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.detect_popins_cnn","title":"<code>detect_popins_cnn(df, window_size=64, epochs=10, threshold_multiplier=5.0, batch_size=32, validation_split=0.0, depth_col='Depth (nm)', load_col='Load (\u00b5N)', window=5, trim_edges_enabled=True, trim_margin=None, max_load_trim_enabled=True)</code>","text":"<p>Detect pop-ins using a Convolutional Autoencoder trained on stiffness features.</p> <p>This method uses an unsupervised CNN-based autoencoder to learn a compressed representation of local indentation behavior. It reconstructs short time windows of two features:   - Stiffness difference: rate of change of the slope (d\u00b2Load/dDepth\u00b2)   - Curvature: second derivative of load (d\u00b3Load/dDepth\u00b3)</p> <p>The reconstruction error (mean squared error) is computed between input and output. High reconstruction errors indicate patterns that the model considers unusual\u2014 these are flagged as potential pop-in events.</p> <p>The method uses a sliding window to extract overlapping sequences from the full curve, trains the model on all windows, and flags windows whose error exceeds a dynamic threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data containing load and depth columns.</p> required <code>window_size</code> <code>int</code> <p>Number of time steps per CNN input window.</p> <code>64</code> <code>epochs</code> <code>int</code> <p>Number of training epochs for the autoencoder.</p> <code>10</code> <code>threshold_multiplier</code> <code>float</code> <p>Multiplier for anomaly detection threshold based on std dev.</p> <code>5.0</code> <code>batch_size</code> <code>int</code> <p>Mini-batch size during training.</p> <code>32</code> <code>validation_split</code> <code>float</code> <p>Proportion of data used for validation (0.0 disables validation).</p> <code>0.0</code> <code>depth_col</code> <code>str</code> <p>Column name for depth measurements.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Column name for load measurements.</p> <code>'Load (\u00b5N)'</code> <code>window</code> <code>int</code> <p>Size of the moving window used for stiffness calculation.</p> <code>5</code> <code>trim_edges_enabled</code> <code>bool</code> <p>If True, trims the first <code>window</code> elements</p> <code>True</code> <code>trim_margin</code> <code>int or None</code> <p>Number of elements to trim from the start.</p> <code>None</code> <code>max_load_trim_enabled</code> <code>bool</code> <p>If True, masks out any anomalies after the maximum load point. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Original DataFrame with a new boolean column: - \"popin_cnn\": True for detected anomalies, False otherwise.     - Only pre-max-load anomalies are returned to focus on loading-phase events. If <code>max_load_trim_enabled</code> is True which is the default.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def detect_popins_cnn(\n    df,\n    window_size=64,\n    epochs=10,\n    threshold_multiplier=5.0,\n    batch_size=32,\n    validation_split=0.0,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n    window=5,\n    trim_edges_enabled=True,\n    trim_margin=None,\n    max_load_trim_enabled=True,\n):\n    \"\"\"\n    Detect pop-ins using a Convolutional Autoencoder trained on stiffness features.\n\n    This method uses an unsupervised CNN-based autoencoder to learn a compressed\n    representation of local indentation behavior. It reconstructs short time windows\n    of two features:\n      - Stiffness difference: rate of change of the slope (d\u00b2Load/dDepth\u00b2)\n      - Curvature: second derivative of load (d\u00b3Load/dDepth\u00b3)\n\n    The reconstruction error (mean squared error) is computed between input and output.\n    High reconstruction errors indicate patterns that the model considers unusual\u2014\n    these are flagged as potential pop-in events.\n\n    The method uses a sliding window to extract overlapping sequences from the full curve,\n    trains the model on all windows, and flags windows whose error exceeds a dynamic threshold.\n\n    Args:\n        df (DataFrame): Input indentation data containing load and depth columns.\n        window_size (int): Number of time steps per CNN input window.\n        epochs (int): Number of training epochs for the autoencoder.\n        threshold_multiplier (float): Multiplier for anomaly detection threshold based on std dev.\n        batch_size (int): Mini-batch size during training.\n        validation_split (float): Proportion of data used for validation (0.0 disables validation).\n        depth_col (str): Column name for depth measurements.\n        load_col (str): Column name for load measurements.\n        window (int): Size of the moving window used for stiffness calculation.\n        trim_edges_enabled (bool): If True, trims the first `window` elements\n        trim_margin (int or None): Number of elements to trim from the start.\n        max_load_trim_enabled (bool): If True, masks out any anomalies after the maximum load point. Default is True.\n\n    Returns:\n        DataFrame: Original DataFrame with a new boolean column:\n            - \"popin_cnn\": True for detected anomalies, False otherwise.\n                - Only pre-max-load anomalies are returned to focus on loading-phase events. If `max_load_trim_enabled` is True which is the default.\n    \"\"\"\n    df2 = compute_features(df, depth_col, load_col, window)\n    X = df2[[\"stiff_diff\", \"curvature\"]].fillna(0).values\n    W = np.array([X[i : i + window_size] for i in range(len(X) - window_size)])\n\n    ae = build_cnn_autoencoder(window_size, 2)\n    ae.compile(optimizer=\"adam\", loss=\"mse\")\n    ae.fit(\n        W,\n        W,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_split=validation_split,\n        verbose=0,\n        callbacks=(\n            [EarlyStopping(patience=3, restore_best_weights=True)]\n            if validation_split &gt; 0\n            else None\n        ),\n    )\n\n    W_pred = ae.predict(W, verbose=0)\n    errors = np.mean((W - W_pred) ** 2, axis=(1, 2))\n    threshold = errors.mean() + threshold_multiplier * errors.std()\n\n    flags = np.zeros(len(X), dtype=bool)\n    flags[window_size // 2 : -window_size // 2] = errors &gt; threshold\n    if trim_edges_enabled:\n        margin = trim_margin if trim_margin is not None else max(10, window)\n        flags = trim_edges(flags, margin=margin)\n\n    if max_load_trim_enabled:\n        # Mask out anything *after* max load\n        max_idx = find_max_load_index(df, load_col)\n        flags[max_idx + 1 :] = False\n\n    df2[\"popin_cnn\"] = flags\n    logger.info(f\"CNN flagged {df2['popin_cnn'].sum()} anomalies\")\n    return df2\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.detect_popins_fd_fourier","title":"<code>detect_popins_fd_fourier(df, threshold=3.0, spacing=1.0, trim_edges_enabled=True, trim_margin=None, load_col='Load (\u00b5N)', max_load_trim_enabled=True)</code>","text":"<p>Detect pop-ins by estimating the derivative of Load using a Fourier spectral method.</p> <p>This method computes the first derivative in the frequency domain using the Fourier Transform. The basic idea is that differentiation in the time domain corresponds to multiplying by a frequency component in the Fourier domain:</p> <pre><code>dLoad/dDepth \u2248 IFFT( i * 2\u03c0 * frequency * FFT(Load) )\n</code></pre> <p>The inverse FFT (IFFT) is then used to convert the differentiated signal back into the spatial domain. IFFT takes frequency-domain data and reconstructs the original time-domain (or spatial) signal.</p> <p>Anomalies are flagged where the resulting derivative deviates from the mean by more than a given number of standard deviations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data.</p> required <code>threshold</code> <code>float</code> <p>Std deviation multiplier to flag anomalies in derivative.</p> <code>3.0</code> <code>spacing</code> <code>float</code> <p>Spacing between data points (in nm or similar units).</p> <code>1.0</code> <code>trim_edges_enabled</code> <code>bool</code> <p>If True, trims the first <code>window</code> elements</p> <code>True</code> <code>trim_margin</code> <code>int or None</code> <p>Number of elements to trim from the start.</p> <code>None</code> <code>load_col</code> <code>str</code> <p>Column name for load data.</p> <code>'Load (\u00b5N)'</code> <code>max_load_trim_enabled</code> <code>bool</code> <p>If True, masks out any anomalies after the maximum load point. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Original DataFrame with a new boolean column: - \"popin_fd\": True for detected anomalies, False otherwise.     - Only pre-max-load anomalies are returned to focus on loading-phase events. If <code>max_load_trim_enabled</code> is True which is the default.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def detect_popins_fd_fourier(\n    df,\n    threshold=3.0,\n    spacing=1.0,\n    trim_edges_enabled=True,\n    trim_margin=None,\n    load_col=\"Load (\u00b5N)\",\n    max_load_trim_enabled=True,\n):\n    \"\"\"\n    Detect pop-ins by estimating the derivative of Load using a Fourier spectral method.\n\n    This method computes the first derivative in the frequency domain using the Fourier Transform.\n    The basic idea is that differentiation in the time domain corresponds to multiplying by a frequency\n    component in the Fourier domain:\n\n        dLoad/dDepth \u2248 IFFT( i * 2\u03c0 * frequency * FFT(Load) )\n\n    The inverse FFT (IFFT) is then used to convert the differentiated signal back into the spatial domain.\n    IFFT takes frequency-domain data and reconstructs the original time-domain (or spatial) signal.\n\n    Anomalies are flagged where the resulting derivative deviates from the mean by more than a\n    given number of standard deviations.\n\n    Args:\n        df (DataFrame): Input indentation data.\n        threshold (float): Std deviation multiplier to flag anomalies in derivative.\n        spacing (float): Spacing between data points (in nm or similar units).\n        trim_edges_enabled (bool): If True, trims the first `window` elements\n        trim_margin (int or None): Number of elements to trim from the start.\n        load_col (str): Column name for load data.\n        max_load_trim_enabled (bool): If True, masks out any anomalies after the maximum load point. Default is True.\n\n    Returns:\n        DataFrame: Original DataFrame with a new boolean column:\n            - \"popin_fd\": True for detected anomalies, False otherwise.\n                - Only pre-max-load anomalies are returned to focus on loading-phase events. If `max_load_trim_enabled` is True which is the default.\n    \"\"\"\n    load = df[load_col].values\n    fft_load = np.fft.fft(load)\n    freqs = np.fft.fftfreq(len(load), d=spacing)\n    derivative = np.real(np.fft.ifft(1j * 2 * np.pi * freqs * fft_load))\n    anomalies = np.abs(derivative - np.mean(derivative)) &gt; threshold * np.std(\n        derivative\n    )\n    if trim_edges_enabled:\n        margin = trim_margin if trim_margin is not None else 10\n        anomalies = trim_edges(anomalies, margin=margin)\n\n    if max_load_trim_enabled:\n        # Mask out anything *after* max load\n        max_idx = find_max_load_index(df, load_col)\n        anomalies[max_idx + 1 :] = False\n\n    df2 = df.copy()\n    df2[\"popin_fd\"] = anomalies\n    logger.info(f\"Fourier spectral method flagged {anomalies.sum()} anomalies\")\n    return df2\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.detect_popins_iforest","title":"<code>detect_popins_iforest(df, contamination=0.001, random_state=None, depth_col='Depth (nm)', load_col='Load (\u00b5N)', window=5, trim_edges_enabled=True, trim_margin=None, max_load_trim_enabled=True)</code>","text":"<p>Detect pop-ins using Isolation Forest based on local stiffness and curvature.</p> This method computes two time-series features <ul> <li>Stiffness difference: the rate of change in the slope of the load\u2013depth curve</li> <li>Curvature: the second derivative of the load curve (change in stiffness difference)</li> </ul> <p>It then applies the Isolation Forest algorithm from scikit-learn, which isolates anomalies by recursively partitioning the feature space. Points that require fewer partitions to isolate are more likely to be outliers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Indentation dataset containing load and depth columns.</p> required <code>contamination</code> <code>float</code> <p>Proportion of expected anomalies in the dataset.</p> <code>0.001</code> <code>random_state</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>depth_col</code> <code>str</code> <p>Name of the depth column.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Name of the load column.</p> <code>'Load (\u00b5N)'</code> <code>window</code> <code>int</code> <p>Size of the sliding window used to compute stiffness.</p> <code>5</code> <code>trim_edges_enabled</code> <code>bool</code> <p>If True, trims the first <code>window</code> elements</p> <code>True</code> <code>trim_margin</code> <code>int or None</code> <p>Number of elements to trim from the start.</p> <code>None</code> <code>max_load_trim_enabled</code> <code>bool</code> <p>If True, masks out any anomalies after the maximum load point. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>A copy of the original DataFrame with a new boolean column: - \"popin_iforest\": True for detected pop-ins (anomalies), False otherwise.     - Only pre-max-load anomalies are returned to focus on loading-phase events. If <code>max_load_trim_enabled</code> is True which is the default.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def detect_popins_iforest(\n    df,\n    contamination=0.001,\n    random_state=None,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n    window=5,\n    trim_edges_enabled=True,\n    trim_margin=None,\n    max_load_trim_enabled=True,\n):\n    \"\"\"\n    Detect pop-ins using Isolation Forest based on local stiffness and curvature.\n\n    This method computes two time-series features:\n      - Stiffness difference: the rate of change in the slope of the load\u2013depth curve\n      - Curvature: the second derivative of the load curve (change in stiffness difference)\n\n    It then applies the Isolation Forest algorithm from scikit-learn, which isolates\n    anomalies by recursively partitioning the feature space. Points that require fewer\n    partitions to isolate are more likely to be outliers.\n\n    Args:\n        df (DataFrame): Indentation dataset containing load and depth columns.\n        contamination (float): Proportion of expected anomalies in the dataset.\n        random_state (int or None): Random seed for reproducibility.\n        depth_col (str): Name of the depth column.\n        load_col (str): Name of the load column.\n        window (int): Size of the sliding window used to compute stiffness.\n        trim_edges_enabled (bool): If True, trims the first `window` elements\n        trim_margin (int or None): Number of elements to trim from the start.\n        max_load_trim_enabled (bool): If True, masks out any anomalies after the maximum load point. Default is True.\n\n    Returns:\n        DataFrame: A copy of the original DataFrame with a new boolean column:\n            - \"popin_iforest\": True for detected pop-ins (anomalies), False otherwise.\n                - Only pre-max-load anomalies are returned to focus on loading-phase events. If `max_load_trim_enabled` is True which is the default.\n    \"\"\"\n    df2 = compute_features(df, depth_col, load_col, window)\n    iso = IsolationForest(contamination=contamination, random_state=random_state)\n    features = df2[[\"stiff_diff\", \"curvature\"]].fillna(0)\n    preds = iso.fit_predict(features) == -1\n    if trim_edges_enabled:\n        margin = trim_margin if trim_margin is not None else max(10, window)\n        preds = trim_edges(preds, margin=margin)\n\n    if max_load_trim_enabled:\n        # Mask out anything *after* max load\n        max_idx = find_max_load_index(df, load_col)\n        preds[max_idx + 1 :] = False\n\n    df2[\"popin_iforest\"] = preds\n    logger.info(f\"IsolationForest flagged {df2['popin_iforest'].sum()} anomalies\")\n    return df2\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.detect_popins_savgol","title":"<code>detect_popins_savgol(df, window_length=11, polyorder=2, threshold=3.0, deriv=1, load_col='Load (\u00b5N)', trim_edges_enabled=True, trim_margin=None, max_load_trim_enabled=True)</code>","text":"<p>Detect pop-ins using Savitzky-Golay filtered derivatives.</p> <p>This method smooths the load data using a polynomial filter and computes its derivative. Anomalies are flagged where the derivative differs significantly from its mean value.</p> The steps are <ol> <li>Apply Savitzky-Golay filter to compute the derivative (e.g., velocity or acceleration)</li> <li>Flag points where |derivative - mean| &gt; threshold * std deviation</li> </ol> <p>The Savitzky-Golay filter works by fitting successive subsets of adjacent data points with a low-degree polynomial using linear least squares.</p> <p>Parameters:</p> Name Type Description Default <code>window_length</code> <code>int</code> <p>Length of the filter window (must be odd).</p> <code>11</code> <code>polyorder</code> <code>int</code> <p>Order of polynomial for smoothing.</p> <code>2</code> <code>threshold</code> <code>float</code> <p>Threshold in standard deviations for detecting anomalies.</p> <code>3.0</code> <code>deriv</code> <code>int</code> <p>Order of derivative to compute (default is 1 for first derivative).</p> <code>1</code> <code>load_col</code> <code>str</code> <p>Column name for load data.</p> <code>'Load (\u00b5N)'</code> <code>trim_edges_enabled</code> <code>bool</code> <p>If True, trims the first <code>window</code> elements</p> <code>True</code> <code>trim_margin</code> <code>int or None</code> <p>Number of elements to trim from the start.</p> <code>None</code> <code>max_load_trim_enabled</code> <code>bool</code> <p>If True, masks out any anomalies after the maximum load point. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <ul> <li>DataFrame: Original dataframe with a new boolean column:</li> <li>\"popin_savgol\": True for detected anomalies, False otherwise.<ul> <li>Only pre-max-load anomalies are returned to focus on loading-phase events. If <code>max_load_trim_enabled</code> is True which is the default.</li> </ul> </li> </ul> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def detect_popins_savgol(\n    df,\n    window_length=11,\n    polyorder=2,\n    threshold=3.0,\n    deriv=1,\n    load_col=\"Load (\u00b5N)\",\n    trim_edges_enabled=True,\n    trim_margin=None,\n    max_load_trim_enabled=True,\n):\n    \"\"\"\n    Detect pop-ins using Savitzky-Golay filtered derivatives.\n\n    This method smooths the load data using a polynomial filter and computes its derivative.\n    Anomalies are flagged where the derivative differs significantly from its mean value.\n\n    The steps are:\n      1. Apply Savitzky-Golay filter to compute the derivative (e.g., velocity or acceleration)\n      2. Flag points where |derivative - mean| &gt; threshold * std deviation\n\n    The Savitzky-Golay filter works by fitting successive subsets of adjacent data points\n    with a low-degree polynomial using linear least squares.\n\n    Args:\n        window_length (int): Length of the filter window (must be odd).\n        polyorder (int): Order of polynomial for smoothing.\n        threshold (float): Threshold in standard deviations for detecting anomalies.\n        deriv (int): Order of derivative to compute (default is 1 for first derivative).\n        load_col (str): Column name for load data.\n        trim_edges_enabled (bool): If True, trims the first `window` elements\n        trim_margin (int or None): Number of elements to trim from the start.\n        max_load_trim_enabled (bool): If True, masks out any anomalies after the maximum load point. Default is True.\n\n    Returns:\n        - DataFrame: Original dataframe with a new boolean column:\n            - \"popin_savgol\": True for detected anomalies, False otherwise.\n                - Only pre-max-load anomalies are returned to focus on loading-phase events. If `max_load_trim_enabled` is True which is the default.\n    \"\"\"\n    derivative = savgol_filter(df[load_col], window_length, polyorder, deriv=deriv)\n    anomalies = np.abs(derivative - np.mean(derivative)) &gt; threshold * np.std(\n        derivative\n    )\n    if trim_edges_enabled:\n        margin = trim_margin if trim_margin is not None else max(10, window_length)\n        anomalies = trim_edges(anomalies, margin=margin)\n\n    if max_load_trim_enabled:\n        # Mask out anything *after* max load\n        max_idx = find_max_load_index(df, load_col)\n        anomalies[max_idx + 1 :] = False\n\n    df2 = df.copy()\n    df2[\"popin_savgol\"] = anomalies\n    logger.info(f\"Savitzky-Golay flagged {anomalies.sum()} anomalies\")\n    return df2\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.find_max_load_index","title":"<code>find_max_load_index(df, load_col='Load (\u00b5N)')</code>","text":"<p>Find the index of the maximum load point in the indentation curve.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data.</p> required <code>load_col</code> <code>str</code> <p>Column name for the load data.</p> <code>'Load (\u00b5N)'</code> <p>Returns:</p> Name Type Description <code>int</code> <p>Index of the maximum load value.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def find_max_load_index(df, load_col=\"Load (\u00b5N)\"):\n    \"\"\"\n    Find the index of the maximum load point in the indentation curve.\n\n    Args:\n        df (DataFrame): Input indentation data.\n        load_col (str): Column name for the load data.\n\n    Returns:\n        int: Index of the maximum load value.\n    \"\"\"\n    return df[load_col].idxmax()\n</code></pre>"},{"location":"reference/merrypopins.locate/#merrypopins.locate.trim_edges","title":"<code>trim_edges(series, margin)</code>","text":"<p>Trim the first <code>margin</code> elements of a pandas Series. This is useful for removing edge effects in time-series data where the first few points may not be reliable. Args:     series (pd.Series): Input time-series data.     margin (int): Number of elements to trim from the start. Returns:     pd.Series: A copy of the input series with the first <code>margin</code> elements set to False.</p> Source code in <code>src/merrypopins/locate.py</code> <pre><code>def trim_edges(series, margin):\n    \"\"\"\n    Trim the first `margin` elements of a pandas Series.\n    This is useful for removing edge effects in time-series data where\n    the first few points may not be reliable.\n    Args:\n        series (pd.Series): Input time-series data.\n        margin (int): Number of elements to trim from the start.\n    Returns:\n        pd.Series: A copy of the input series with the first `margin` elements set to False.\n    \"\"\"\n    trimmed = series.copy()\n    trimmed[:margin] = False\n    return trimmed\n</code></pre>"},{"location":"reference/merrypopins.make_dataset/","title":"make_dataset","text":""},{"location":"reference/merrypopins.make_dataset/#merrypopins.make_dataset--make_datasetpy","title":"make_dataset.py","text":"<p>This module provides a function to execute the full Merrypopins pipeline, from loading the dataset to preprocessing, locating pop-ins, and visualizing the results.</p> <p>It integrates various methods for pop-in detection and saves the results as a DataFrame.</p> <p>Provides: - <code>merrypopins_pipeline</code>: A function that orchestrates the entire process. - Saves visualizations of the detected pop-ins. - Returns a DataFrame with all annotations.</p>"},{"location":"reference/merrypopins.make_dataset/#merrypopins.make_dataset.merrypopins_pipeline","title":"<code>merrypopins_pipeline(txt_path, iforest_contamination=0.001, iforest_random_state=None, cnn_window_size=64, cnn_epochs=10, cnn_threshold_multiplier=5.0, cnn_batch_size=32, cnn_validation_split=0.0, fd_threshold=3.0, fd_spacing=1.0, savgol_window_length=11, savgol_polyorder=2, savgol_threshold=3.0, sg_deriv_order=1, stiffness_window=5, trim_edges_enabled=True, trim_margin=None, max_load_trim_enabled=True, use_iforest=True, use_cnn=True, use_fd=True, use_savgol=True, depth_col='Depth (nm)', load_col='Load (\u00b5N)', save_plot_dir=Path('visualisations'))</code>","text":"<p>Executes the full Merrypopins pipeline: load -&gt; preprocess -&gt; locate -&gt; visualize.</p> <p>Parameters:</p> Name Type Description Default <code>txt_path</code> <code>Path or str</code> <p>Path to the indentation .txt file.</p> required <code>iforest_contamination</code> <code>float</code> <p>Contamination level for IsolationForest.</p> <code>0.001</code> <code>iforest_random_state</code> <code>int</code> <p>Random seed for IsolationForest.</p> <code>None</code> <code>cnn_window_size</code> <code>int</code> <p>CNN autoencoder window size.</p> <code>64</code> <code>cnn_epochs</code> <code>int</code> <p>CNN training epochs.</p> <code>10</code> <code>cnn_threshold_multiplier</code> <code>float</code> <p>Threshold multiplier for CNN.</p> <code>5.0</code> <code>cnn_batch_size</code> <code>int</code> <p>Batch size for CNN training.</p> <code>32</code> <code>cnn_validation_split</code> <code>float</code> <p>Fraction of data for validation.</p> <code>0.0</code> <code>fd_threshold</code> <code>float</code> <p>Threshold for finite difference method.</p> <code>3.0</code> <code>fd_spacing</code> <code>float</code> <p>Sampling interval for Fourier derivative.</p> <code>1.0</code> <code>savgol_window_length</code> <code>int</code> <p>Window length for Savitzky-Golay filter.</p> <code>11</code> <code>savgol_polyorder</code> <code>int</code> <p>Polynomial order for Savitzky-Golay.</p> <code>2</code> <code>savgol_threshold</code> <code>float</code> <p>Threshold for Savitzky-Golay.</p> <code>3.0</code> <code>sg_deriv_order</code> <code>int</code> <p>Derivative order for Savitzky-Golay.</p> <code>1</code> <code>stiffness_window</code> <code>int</code> <p>Smoothing window for stiffness calculation.</p> <code>5</code> <code>trim_edges_enabled</code> <code>bool</code> <p>Whether to trim pop-ins at curve edges.</p> <code>True</code> <code>trim_margin</code> <code>int</code> <p>Margin to trim detections at beginning.</p> <code>None</code> <code>max_load_trim_enabled</code> <code>bool</code> <p>Whether to trim pop-ins based on max load.</p> <code>True</code> <code>use_iforest</code> <code>bool</code> <p>Enable IsolationForest detection.</p> <code>True</code> <code>use_cnn</code> <code>bool</code> <p>Enable CNN detection.</p> <code>True</code> <code>use_fd</code> <code>bool</code> <p>Enable Fourier detection.</p> <code>True</code> <code>use_savgol</code> <code>bool</code> <p>Enable Savitzky-Golay detection.</p> <code>True</code> <code>depth_col</code> <code>str</code> <p>Column name for depth.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Column name for load.</p> <code>'Load (\u00b5N)'</code> <code>save_plot_dir</code> <code>Path</code> <p>Directory where plots will be saved.</p> <code>Path('visualisations')</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Final DataFrame with all annotations.</p> Source code in <code>src/merrypopins/make_dataset.py</code> <pre><code>def merrypopins_pipeline(\n    txt_path,\n    iforest_contamination=0.001,\n    iforest_random_state=None,\n    cnn_window_size=64,\n    cnn_epochs=10,\n    cnn_threshold_multiplier=5.0,\n    cnn_batch_size=32,\n    cnn_validation_split=0.0,\n    fd_threshold=3.0,\n    fd_spacing=1.0,\n    savgol_window_length=11,\n    savgol_polyorder=2,\n    savgol_threshold=3.0,\n    sg_deriv_order=1,\n    stiffness_window=5,\n    trim_edges_enabled=True,\n    trim_margin=None,\n    max_load_trim_enabled=True,\n    use_iforest=True,\n    use_cnn=True,\n    use_fd=True,\n    use_savgol=True,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n    save_plot_dir=Path(\"visualisations\"),\n):\n    \"\"\"\n    Executes the full Merrypopins pipeline: load -&gt; preprocess -&gt; locate -&gt; visualize.\n\n    Args:\n        txt_path (Path or str): Path to the indentation .txt file.\n        iforest_contamination (float): Contamination level for IsolationForest.\n        iforest_random_state (int): Random seed for IsolationForest.\n        cnn_window_size (int): CNN autoencoder window size.\n        cnn_epochs (int): CNN training epochs.\n        cnn_threshold_multiplier (float): Threshold multiplier for CNN.\n        cnn_batch_size (int): Batch size for CNN training.\n        cnn_validation_split (float): Fraction of data for validation.\n        fd_threshold (float): Threshold for finite difference method.\n        fd_spacing (float): Sampling interval for Fourier derivative.\n        savgol_window_length (int): Window length for Savitzky-Golay filter.\n        savgol_polyorder (int): Polynomial order for Savitzky-Golay.\n        savgol_threshold (float): Threshold for Savitzky-Golay.\n        sg_deriv_order (int): Derivative order for Savitzky-Golay.\n        stiffness_window (int): Smoothing window for stiffness calculation.\n        trim_edges_enabled (bool): Whether to trim pop-ins at curve edges.\n        trim_margin (int): Margin to trim detections at beginning.\n        max_load_trim_enabled (bool): Whether to trim pop-ins based on max load.\n        use_iforest (bool): Enable IsolationForest detection.\n        use_cnn (bool): Enable CNN detection.\n        use_fd (bool): Enable Fourier detection.\n        use_savgol (bool): Enable Savitzky-Golay detection.\n        depth_col (str): Column name for depth.\n        load_col (str): Column name for load.\n        save_plot_dir (Path): Directory where plots will be saved.\n\n    Returns:\n        DataFrame: Final DataFrame with all annotations.\n    \"\"\"\n    txt_path = Path(txt_path)\n    df = load_txt(txt_path)\n    df_pre = default_preprocess(df)\n    df_loc = default_locate(\n        df_pre,\n        iforest_contamination=iforest_contamination,\n        iforest_random_state=iforest_random_state,\n        cnn_window_size=cnn_window_size,\n        cnn_epochs=cnn_epochs,\n        cnn_threshold_multiplier=cnn_threshold_multiplier,\n        cnn_batch_size=cnn_batch_size,\n        cnn_validation_split=cnn_validation_split,\n        fd_threshold=fd_threshold,\n        fd_spacing=fd_spacing,\n        savgol_window_length=savgol_window_length,\n        savgol_polyorder=savgol_polyorder,\n        savgol_threshold=savgol_threshold,\n        sg_deriv_order=sg_deriv_order,\n        stiffness_window=stiffness_window,\n        trim_edges_enabled=trim_edges_enabled,\n        trim_margin=trim_margin,\n        max_load_trim_enabled=max_load_trim_enabled,\n        use_iforest=use_iforest,\n        use_cnn=use_cnn,\n        use_fd=use_fd,\n        use_savgol=use_savgol,\n        depth_col=depth_col,\n        load_col=load_col,\n    )\n\n    save_plot_dir.mkdir(parents=True, exist_ok=True)\n    plot_path = save_plot_dir / f\"{txt_path.stem}.png\"\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(\n        df_loc[depth_col],\n        df_loc[load_col],\n        label=\"Preprocessed\",\n        alpha=0.4,\n        color=\"orange\",\n    )\n\n    colors = {\n        \"popin_iforest\": \"red\",\n        \"popin_cnn\": \"purple\",\n        \"popin_fd\": \"darkorange\",\n        \"popin_savgol\": \"green\",\n    }\n    markers = {\n        \"popin_iforest\": \"^\",\n        \"popin_cnn\": \"v\",\n        \"popin_fd\": \"x\",\n        \"popin_savgol\": \"D\",\n    }\n\n    for method, color in colors.items():\n        mdf = df_loc[df_loc[method]]\n        plt.scatter(\n            mdf[depth_col],\n            mdf[load_col],\n            c=color,\n            label=method.replace(\"popin_\", \"\").capitalize(),\n            marker=markers[method],\n            alpha=0.7,\n        )\n\n    confident = df_loc[df_loc[\"popin_confident\"]]\n    plt.scatter(\n        confident[depth_col],\n        confident[load_col],\n        edgecolors=\"k\",\n        facecolors=\"none\",\n        label=\"Majority Vote (2+)\",\n        s=100,\n        linewidths=1.5,\n    )\n\n    plt.xlabel(depth_col)\n    plt.ylabel(load_col)\n    plt.title(f\"Pop-in Detection: {txt_path.stem}\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(plot_path)\n    plt.close()\n\n    return df_loc\n</code></pre>"},{"location":"reference/merrypopins.preprocess/","title":"preprocess","text":""},{"location":"reference/merrypopins.preprocess/#merrypopins.preprocess--preprocesspy","title":"preprocess.py","text":"<p>Provides pre-processing functions for indentation datasets.</p> <p>Functions:</p> Name Description <code>- remove_pre_min_load</code> <p>Remove all points up to the minimum Load point.</p> <code>- rescale_data</code> <p>Automatically detect contact point and rescale Depth.</p> <code>- finalise_contact_index</code> <p>Optionally trim and/or flag the contact point.</p> <code>- default_preprocess</code> <p>Recommended preprocessing pipeline.</p> Usage <p>from merrypopins.preprocess import (     remove_pre_min_load,     rescale_data,     finalise_contact_index,     default_preprocess )</p>"},{"location":"reference/merrypopins.preprocess/#merrypopins.preprocess.default_preprocess","title":"<code>default_preprocess(df)</code>","text":"<p>Default preprocessing pipeline using recommended settings.</p> Steps <ul> <li>Remove early data up to the minimum Load point</li> <li>Automatically detect contact and rescale Depth</li> <li>Remove Depth &lt; 0 rows and flag the contact point</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Raw indentation data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Preprocessed DataFrame.</p> Source code in <code>src/merrypopins/preprocess.py</code> <pre><code>def default_preprocess(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Default preprocessing pipeline using recommended settings.\n\n    Steps:\n        - Remove early data up to the minimum Load point\n        - Automatically detect contact and rescale Depth\n        - Remove Depth &lt; 0 rows and flag the contact point\n\n    Args:\n        df (pd.DataFrame): Raw indentation data.\n\n    Returns:\n        pd.DataFrame: Preprocessed DataFrame.\n    \"\"\"\n    df = remove_pre_min_load(df, load_col=\"Load (\u00b5N)\")\n    df = rescale_data(\n        df,\n        depth_col=\"Depth (nm)\",\n        load_col=\"Load (\u00b5N)\",\n        N_baseline=50,\n        k=5,\n        window_length=11,\n        polyorder=2,\n    )\n    df = finalise_contact_index(\n        df,\n        depth_col=\"Depth (nm)\",\n        remove_pre_contact=True,\n        add_flag_column=True,\n        flag_column=\"contact_point\",\n    )\n    return df\n</code></pre>"},{"location":"reference/merrypopins.preprocess/#merrypopins.preprocess.finalise_contact_index","title":"<code>finalise_contact_index(df, depth_col='Depth (nm)', remove_pre_contact=True, add_flag_column=True, flag_column='contact_point')</code>","text":"<p>Optionally remove all rows before contact (Depth &lt; 0) and/or flag the first contact point.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Rescaled DataFrame.</p> required <code>depth_col</code> <code>str</code> <p>Depth column name.</p> <code>'Depth (nm)'</code> <code>remove_pre_contact</code> <code>bool</code> <p>If True, remove rows with Depth &lt; 0. Default is True.</p> <code>True</code> <code>add_flag_column</code> <code>bool</code> <p>If True, add a column marking the contact index. Default is True.</p> <code>True</code> <code>flag_column</code> <code>str</code> <p>Name of the column used to flag the contact point. Default column name is \"contact_point\".</p> <code>'contact_point'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame after trimming/flagging contact point.</p> Source code in <code>src/merrypopins/preprocess.py</code> <pre><code>def finalise_contact_index(\n    df: pd.DataFrame,\n    depth_col: str = \"Depth (nm)\",\n    remove_pre_contact: bool = True,\n    add_flag_column: bool = True,\n    flag_column: str = \"contact_point\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Optionally remove all rows before contact (Depth &lt; 0) and/or flag the first contact point.\n\n    Args:\n        df (pd.DataFrame): Rescaled DataFrame.\n        depth_col (str): Depth column name.\n        remove_pre_contact (bool): If True, remove rows with Depth &lt; 0. Default is True.\n        add_flag_column (bool): If True, add a column marking the contact index. Default is True.\n        flag_column (str): Name of the column used to flag the contact point. Default column name is \"contact_point\".\n\n    Returns:\n        pd.DataFrame: DataFrame after trimming/flagging contact point.\n    \"\"\"\n    df2 = df.copy()\n    contact_idx = df2[df2[depth_col] &gt;= 0].index.min()\n\n    if pd.isna(contact_idx):\n        if add_flag_column:\n            df2[flag_column] = False\n        if remove_pre_contact:\n            df2 = df2.iloc[0:0]\n        logger.warning(\"No Depth &gt;= 0 found; contact index undefined.\")\n        return df2\n\n    if add_flag_column:\n        df2[flag_column] = False\n        df2.loc[contact_idx, flag_column] = True\n        logger.info(f\"Flagged contact point at index {contact_idx}\")\n\n    if remove_pre_contact:\n        df2 = df2.loc[contact_idx:].reset_index(drop=True)\n        logger.info(f\"Removed {contact_idx} rows before contact point (Depth &lt; 0)\")\n\n    return df2\n</code></pre>"},{"location":"reference/merrypopins.preprocess/#merrypopins.preprocess.remove_pre_min_load","title":"<code>remove_pre_min_load(df, load_col='Load (\u00b5N)')</code>","text":"<p>Remove all points up to and including the minimum Load point.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>load_col</code> <code>str</code> <p>Load column name.</p> <code>'Load (\u00b5N)'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Cleaned DataFrame.</p> Source code in <code>src/merrypopins/preprocess.py</code> <pre><code>def remove_pre_min_load(df: pd.DataFrame, load_col=\"Load (\u00b5N)\") -&gt; pd.DataFrame:\n    \"\"\"\n    Remove all points up to and including the minimum Load point.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        load_col (str): Load column name.\n\n    Returns:\n        pd.DataFrame: Cleaned DataFrame.\n    \"\"\"\n    df2 = df.copy()\n    loads = df2[load_col].values\n    min_idx = np.argmin(loads)\n\n    if min_idx &gt;= len(loads) - 1:\n        logger.warning(\"Minimum at end of data; skipping initial data removal.\")\n        return df2\n\n    df_clean = df2.iloc[min_idx + 1 :].reset_index(drop=True)\n    logger.info(\n        f\"Removed first {min_idx + 1} points up to minimum Load ({loads[min_idx]:.2f})\"\n    )\n    return df_clean\n</code></pre>"},{"location":"reference/merrypopins.preprocess/#merrypopins.preprocess.rescale_data","title":"<code>rescale_data(df, depth_col='Depth (nm)', load_col='Load (\u00b5N)', N_baseline=50, k=5, window_length=11, polyorder=2)</code>","text":"<p>Automatically detect contact point by noise threshold and rescale Depth so contact = 0.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>depth_col</code> <code>str</code> <p>Depth column name.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Load column name.</p> <code>'Load (\u00b5N)'</code> <code>N_baseline</code> <code>int</code> <p>Number of points for baseline noise estimation.</p> <code>50</code> <code>k</code> <code>float</code> <p>Noise multiplier for threshold.</p> <code>5</code> <code>window_length</code> <code>int</code> <p>Smoothing window (must be odd).</p> <code>11</code> <code>polyorder</code> <code>int</code> <p>Polynomial order for smoothing.</p> <code>2</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Rescaled DataFrame.</p> Source code in <code>src/merrypopins/preprocess.py</code> <pre><code>def rescale_data(\n    df: pd.DataFrame,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n    N_baseline=50,\n    k=5,\n    window_length=11,\n    polyorder=2,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Automatically detect contact point by noise threshold and rescale Depth so contact = 0.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        depth_col (str): Depth column name.\n        load_col (str): Load column name.\n        N_baseline (int): Number of points for baseline noise estimation.\n        k (float): Noise multiplier for threshold.\n        window_length (int): Smoothing window (must be odd).\n        polyorder (int): Polynomial order for smoothing.\n\n    Returns:\n        pd.DataFrame: Rescaled DataFrame.\n    \"\"\"\n    df2 = df.copy()\n    loads = df2[load_col].values\n    baseline = loads[:N_baseline]\n    noise_mean, noise_std = baseline.mean(), baseline.std()\n    threshold = noise_mean + k * noise_std\n\n    # Safely calculate smoothing window\n    wl = min(window_length, len(loads) // 2 * 2 - 1)\n\n    if wl &lt;= polyorder:\n        logger.warning(\"Not enough data to smooth: skipping smoothing.\")\n        smooth_loads = loads\n    else:\n        smooth_loads = savgol_filter(loads, window_length=wl, polyorder=polyorder)\n\n    idx = np.argmax(smooth_loads &gt; threshold)\n    if smooth_loads[idx] &lt;= threshold:\n        logger.warning(\n            f\"No crossing above auto-threshold ({threshold:.2f}); skipping rescale.\"\n        )\n        return df2\n\n    shift = df2[depth_col].iloc[idx]\n    df2[depth_col] = df2[depth_col] - shift\n    logger.info(\n        f\"Auto-rescaled at index {idx}, load={smooth_loads[idx]:.2f} &gt; {threshold:.2f}, shift={shift:.1f} nm\"\n    )\n    return df2\n</code></pre>"},{"location":"reference/merrypopins.statistics/","title":"statistics","text":""},{"location":"reference/merrypopins.statistics/#merrypopins.statistics--statisticspy","title":"statistics.py","text":"<p>Extracts statistics from nanoindentation data: - Postprocess located popins - Extract pop-in intervals - Stress\u2013strain transformation Kalidindi &amp; Pathak (2008). - Calculate pop-in statistics (load-depth and stress-strain) - Calculate curve-level summary statistics (load-depth)</p>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.calculate_curve_summary","title":"<code>calculate_curve_summary(df, start_col='start_idx', end_col='end_idx', time_col='Time (s)')</code>","text":"<p>Compute curve-level summary statistics about pop-in activity.</p> <p>This function calculates the number of pop-ins, total pop-in duration, first and last pop-in times, and the average time between consecutive pop-ins.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame that includes pop-in intervals.</p> required <code>start_col,</code> <code>end_col (str</code> <p>Column names for start and end indices of pop-ins.</p> required <code>time_col</code> <code>str</code> <p>Column name for time.</p> <code>'Time (s)'</code> <p>Returns:</p> Type Description <p>pd.Series: Summary metrics: count, total duration, first/last timing, average interval.</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def calculate_curve_summary(\n    df, start_col=\"start_idx\", end_col=\"end_idx\", time_col=\"Time (s)\"\n):\n    \"\"\"\n    Compute curve-level summary statistics about pop-in activity.\n\n    This function calculates the number of pop-ins, total pop-in duration, first and last pop-in times,\n    and the average time between consecutive pop-ins.\n\n    Args:\n        df (pd.DataFrame): DataFrame that includes pop-in intervals.\n        start_col, end_col (str): Column names for start and end indices of pop-ins.\n        time_col (str): Column name for time.\n\n    Returns:\n        pd.Series: Summary metrics: count, total duration, first/last timing, average interval.\n    \"\"\"\n    interval_rows = df.dropna(subset=[start_col, end_col]).copy().reset_index(drop=True)\n    n_popins = len(interval_rows)\n    if n_popins &gt; 0:\n        all_starts = (\n            interval_rows[start_col].astype(int).apply(lambda idx: df.at[idx, time_col])\n        )\n        all_ends = (\n            interval_rows[end_col].astype(int).apply(lambda idx: df.at[idx, time_col])\n        )\n        total_popin_duration = all_ends.max() - all_starts.min()\n        avg_time_between = all_starts.diff().dropna().mean()\n        first_popin_time = all_starts.min()\n        last_popin_time = all_ends.max()\n    else:\n        total_popin_duration = 0.0\n        avg_time_between = None\n        first_popin_time = None\n        last_popin_time = None\n\n    return pd.Series(\n        {\n            \"n_popins\": n_popins,\n            \"total_test_duration\": df[time_col].max() - df[time_col].min(),\n            \"total_popin_duration\": total_popin_duration,\n            \"first_popin_time\": first_popin_time,\n            \"last_popin_time\": last_popin_time,\n            \"avg_time_between_popins\": avg_time_between,\n        }\n    )\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.calculate_popin_statistics","title":"<code>calculate_popin_statistics(df, precursor_stats=True, temporal_stats=True, popin_shape_stats=True, time_col='Time (s)', load_col='Load (\u00b5N)', depth_col='Depth (nm)', start_col='start_idx', end_col='end_idx', before_window=0.5, after_window=0.5)</code>","text":"<p>Compute descriptive statistics for each detected pop-in.</p> <p>This function calculates time-based, precursor-based, and shape-based features for each interval where a pop-in occurred (based on start and end index).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame with indentation data and interval metadata.</p> required <code>precursor_stats</code> <code>bool</code> <p>Whether to calculate average dLoad and slope before the pop-in.</p> <code>True</code> <code>temporal_stats</code> <code>bool</code> <p>Whether to calculate duration and inter-event timing features.</p> <code>True</code> <code>popin_shape_stats</code> <code>bool</code> <p>Whether to compute shape-based features like velocity and curvature.</p> <code>True</code> <code>time_col,</code> <code>load_col, depth_col (str</code> <p>Column names for time, load, and depth.</p> required <code>start_col,</code> <code>end_col (str</code> <p>Column names for the start and end index of pop-in intervals.</p> required <code>before_window,</code> <code>after_window (float</code> <p>Time window in seconds to use for context before/after the pop-in.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: Original DataFrame with per-pop-in statistics added (NaNs elsewhere).</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def calculate_popin_statistics(\n    df,\n    precursor_stats=True,\n    temporal_stats=True,\n    popin_shape_stats=True,\n    time_col=\"Time (s)\",\n    load_col=\"Load (\u00b5N)\",\n    depth_col=\"Depth (nm)\",\n    start_col=\"start_idx\",\n    end_col=\"end_idx\",\n    before_window=0.5,\n    after_window=0.5,\n):\n    \"\"\"\n    Compute descriptive statistics for each detected pop-in.\n\n    This function calculates time-based, precursor-based, and shape-based features\n    for each interval where a pop-in occurred (based on start and end index).\n\n    Args:\n        df (pd.DataFrame): Input DataFrame with indentation data and interval metadata.\n        precursor_stats (bool): Whether to calculate average dLoad and slope before the pop-in.\n        temporal_stats (bool): Whether to calculate duration and inter-event timing features.\n        popin_shape_stats (bool): Whether to compute shape-based features like velocity and curvature.\n        time_col, load_col, depth_col (str): Column names for time, load, and depth.\n        start_col, end_col (str): Column names for the start and end index of pop-in intervals.\n        before_window, after_window (float): Time window in seconds to use for context before/after the pop-in.\n\n    Returns:\n        pd.DataFrame: Original DataFrame with per-pop-in statistics added (NaNs elsewhere).\n    \"\"\"\n    df = df.copy()\n    interval_rows = df.dropna(subset=[start_col, end_col]).copy().reset_index(drop=True)\n    df[\"dLoad\"] = df[load_col].diff() / df[time_col].diff()\n    results = []\n\n    for i, row in interval_rows.iterrows():\n        start_idx = int(row[start_col])\n        end_idx = int(row[end_col])\n        start_time = df.at[start_idx, time_col]\n        end_time = df.at[end_idx, time_col]\n\n        # Ensure the 'before' window captures data before the pop-in\n        before = df[\n            (df[time_col] &gt;= start_time - before_window) &amp; (df[time_col] &lt; start_time)\n        ]\n\n        # Ensure the 'during' window captures data during and after the pop-in\n        during = df[\n            (df[time_col] &gt;= start_time) &amp; (df[time_col] &lt;= end_time + after_window)\n        ]\n\n        record = {\"start_idx\": start_idx, \"end_idx\": end_idx}\n\n        if temporal_stats:\n            record.update(\n                _compute_temporal_stats(\n                    start_time, end_time, interval_rows, i, df, time_col, start_col\n                )\n            )\n        if precursor_stats:\n            record.update(_compute_precursor_stats(before, time_col, load_col))\n        if popin_shape_stats:\n            record.update(\n                _compute_shape_stats(\n                    df, start_idx, end_idx, during, time_col, depth_col\n                )\n            )\n\n        results.append(record)\n\n    stats_df = pd.DataFrame(results)\n    for col in stats_df.columns:\n        if col not in [start_col, end_col]:\n            df[col] = df[start_col].map(stats_df.set_index(\"start_idx\")[col])\n\n    logger.info(f\"Computed pop-in statistics for {len(stats_df)} pop-ins\")\n    return df\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.calculate_stress_strain","title":"<code>calculate_stress_strain(df, depth_col='Depth (nm)', load_col='Load (\u00b5N)', Reff_um=5.323, min_load_uN=2000, smooth_stress=True, smooth_window=11, smooth_polyorder=2, copy_popin_cols=True)</code>","text":"<p>Convert load\u2013depth data to stress\u2013strain using Kalidindi &amp; Pathak (2008) formulas.</p> <p>This function converts indentation data from load and depth measurements to stress and strain values using the Kalidindi &amp; Pathak (2008). approach. It optionally copies pop-in markers from the input DataFrame and filters data based on load. Additionally, stress can be smoothed using the Savitzky-Golay filter. With the current setup, stress-strain data is accurate up to the yield point, after which it becomes increasingly inaccurate. To be expanded upon in a future version.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the indentation data.</p> required <code>depth_col</code> <code>str</code> <p>Column name for the depth data.</p> <code>'Depth (nm)'</code> <code>load_col</code> <code>str</code> <p>Column name for the load data.</p> <code>'Load (\u00b5N)'</code> <code>Reff_um</code> <code>float</code> <p>Effective tip radius in microns.</p> <code>5.323</code> <code>min_load_uN</code> <code>float</code> <p>Minimum load threshold to filter out low-load points (in \u00b5N).</p> <code>2000</code> <code>smooth_stress</code> <code>bool</code> <p>Whether to apply smoothing to the stress signal.</p> <code>True</code> <code>smooth_window</code> <code>int</code> <p>Window size for the Savitzky-Golay filter.</p> <code>11</code> <code>smooth_polyorder</code> <code>int</code> <p>Polynomial order for the Savitzky-Golay filter.</p> <code>2</code> <code>copy_popin_cols</code> <code>bool</code> <p>Whether to copy pop-in markers from the input DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with additional columns for stress, strain, and optionally pop-in markers.</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def calculate_stress_strain(\n    df,\n    depth_col=\"Depth (nm)\",\n    load_col=\"Load (\u00b5N)\",\n    Reff_um=5.323,\n    min_load_uN=2000,\n    smooth_stress=True,\n    smooth_window=11,\n    smooth_polyorder=2,\n    copy_popin_cols=True,\n):\n    \"\"\"\n    Convert load\u2013depth data to stress\u2013strain using Kalidindi &amp; Pathak (2008) formulas.\n\n    This function converts indentation data from load and depth measurements to stress and strain values using\n    the Kalidindi &amp; Pathak (2008). approach. It optionally copies pop-in markers from the input DataFrame and filters\n    data based on load. Additionally, stress can be smoothed using the Savitzky-Golay filter. With the current setup,\n    stress-strain data is accurate up to the yield point, after which it becomes increasingly inaccurate. To be\n    expanded upon in a future version.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the indentation data.\n        depth_col (str): Column name for the depth data.\n        load_col (str): Column name for the load data.\n        Reff_um (float): Effective tip radius in microns.\n        min_load_uN (float): Minimum load threshold to filter out low-load points (in \u00b5N).\n        smooth_stress (bool): Whether to apply smoothing to the stress signal.\n        smooth_window (int): Window size for the Savitzky-Golay filter.\n        smooth_polyorder (int): Polynomial order for the Savitzky-Golay filter.\n        copy_popin_cols (bool): Whether to copy pop-in markers from the input DataFrame.\n\n    Returns:\n        pd.DataFrame: DataFrame with additional columns for stress, strain, and optionally pop-in markers.\n    \"\"\"\n    required_cols = [depth_col, load_col, \"Time (s)\"]\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required column(s): {missing_cols}\")\n\n    df = df.copy()\n\n    # Calculate stress and strain before filtering\n    h_m = df[depth_col] * 1e-9\n    P_N = df[load_col] * 1e-6\n    Reff_m = Reff_um * 1e-6\n\n    a = np.sqrt(Reff_m * h_m)\n    df[\"a_contact_m\"] = a\n    df[\"strain\"] = h_m / (2.4 * a)\n    df[\"stress\"] = P_N / (np.pi * a**2) / 1e6  # MPa\n\n    # Copy pop-in flags by index\n    if copy_popin_cols:\n        df[\"popin_start\"] = False\n        df[\"popin_end\"] = False\n        if \"start_idx\" in df.columns:\n            df.loc[df[\"start_idx\"].dropna().astype(int), \"popin_start\"] = True\n        if \"end_idx\" in df.columns:\n            df.loc[df[\"end_idx\"].dropna().astype(int), \"popin_end\"] = True\n        if \"popin_selected\" in df.columns:\n            df[\"popin_selected\"] = df[\"popin_selected\"].fillna(False)\n\n    # Filter by load\n    df_filtered = df[df[load_col] &gt;= min_load_uN].copy()\n\n    if df_filtered.empty:\n        raise ValueError(\"No data points remain after filtering by min_load_uN\")\n\n    # Apply smoothing if needed\n    if smooth_stress and len(df_filtered) &gt;= smooth_window:\n        df_filtered[\"stress\"] = savgol_filter(\n            df_filtered[\"stress\"], smooth_window, smooth_polyorder\n        )\n\n    logger.info(f\"Computed stress\u2013strain for {len(df_filtered)} points\")\n    return df_filtered\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.calculate_stress_strain_statistics","title":"<code>calculate_stress_strain_statistics(df, start_col='start_idx', end_col='end_idx', time_col='Time (s)', stress_col='stress', strain_col='strain', before_window=0.5, precursor_stats=True, temporal_stats=True, shape_stats=True)</code>","text":"<p>Compute statistics for each pop-in in stress\u2013strain space.</p> <p>This function computes various statistics related to stress and strain for each detected pop-in event. It calculates features such as the jump in stress and strain, slope of the stress-strain curve, and temporal statistics.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Data with stress/strain and pop-in intervals.</p> required <code>start_col,</code> <code>end_col (str</code> <p>Columns marking start and end indices of pop-ins.</p> required <code>time_col</code> <code>str</code> <p>Time column.</p> <code>'Time (s)'</code> <code>stress_col,</code> <code>strain_col (str</code> <p>Stress and strain columns.</p> required <code>before_window</code> <code>float</code> <p>Time window to use for precursor features.</p> <code>0.5</code> <code>precursor_stats</code> <code>bool</code> <p>Whether to compute precursor statistics (e.g., slope).</p> <code>True</code> <code>temporal_stats</code> <code>bool</code> <p>Whether to compute temporal statistics (e.g., pop-in duration).</p> <code>True</code> <code>shape_stats</code> <code>bool</code> <p>Whether to compute shape-based statistics (e.g., velocity, curvature).</p> <code>True</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with per-pop-in stress/strain statistics added.</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def calculate_stress_strain_statistics(\n    df,\n    start_col=\"start_idx\",\n    end_col=\"end_idx\",\n    time_col=\"Time (s)\",\n    stress_col=\"stress\",\n    strain_col=\"strain\",\n    before_window=0.5,\n    precursor_stats=True,\n    temporal_stats=True,\n    shape_stats=True,\n):\n    \"\"\"\n    Compute statistics for each pop-in in stress\u2013strain space.\n\n    This function computes various statistics related to stress and strain for each detected pop-in event.\n    It calculates features such as the jump in stress and strain, slope of the stress-strain curve, and temporal statistics.\n\n    Args:\n        df (pd.DataFrame): Data with stress/strain and pop-in intervals.\n        start_col, end_col (str): Columns marking start and end indices of pop-ins.\n        time_col (str): Time column.\n        stress_col, strain_col (str): Stress and strain columns.\n        before_window (float): Time window to use for precursor features.\n        precursor_stats (bool): Whether to compute precursor statistics (e.g., slope).\n        temporal_stats (bool): Whether to compute temporal statistics (e.g., pop-in duration).\n        shape_stats (bool): Whether to compute shape-based statistics (e.g., velocity, curvature).\n\n    Returns:\n        pd.DataFrame: DataFrame with per-pop-in stress/strain statistics added.\n    \"\"\"\n    df = df.copy()\n    interval_rows = df.dropna(subset=[start_col, end_col]).copy().reset_index(drop=True)\n    results = []\n\n    for i, row in interval_rows.iterrows():\n        start_idx = int(row[start_col])\n        end_idx = int(row[end_col])\n        start_time = df.at[start_idx, time_col]\n        end_time = df.at[end_idx, time_col]\n\n        during = df[(df[time_col] &gt;= start_time) &amp; (df[time_col] &lt;= end_time)]\n        before = df[\n            (df[time_col] &gt;= start_time - before_window) &amp; (df[time_col] &lt; start_time)\n        ]\n\n        record = {\"start_idx\": start_idx, \"end_idx\": end_idx}\n\n        if shape_stats:\n            record.update(\n                _compute_stress_strain_jump_stats(\n                    df, start_idx, end_idx, stress_col, strain_col\n                )\n            )\n            record.update(\n                _compute_stress_strain_shape_stats(\n                    during, time_col, stress_col, strain_col\n                )\n            )\n\n        if precursor_stats:\n            record.update(\n                _compute_stress_strain_precursor_stats(\n                    before, time_col, stress_col, strain_col\n                )\n            )\n\n        if temporal_stats:\n            # Add temporal statistics such as pop-in duration and time between events\n            record.update(\n                _compute_temporal_stats(\n                    start_time, end_time, interval_rows, i, df, time_col, start_col\n                )\n            )\n\n        results.append(record)\n\n    stats_df = pd.DataFrame(results)\n    for col in stats_df.columns:\n        if col not in [start_col, end_col]:\n            df[col] = df[start_col].map(stats_df.set_index(\"start_idx\")[col])\n\n    logger.info(f\"Computed stress\u2013strain statistics for {len(stats_df)} pop-ins\")\n    return df\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.default_statistics","title":"<code>default_statistics(df_locate, popin_flag_column='popin', before_window=0.5, after_window=0.5)</code>","text":"<p>Pipeline to compute pop-in statistics from raw located popins.</p> <p>This function extracts relevant columns, selects valid pop-in candidates based on local maxima, extracts intervals for each pop-in event, and calculates descriptive statistics for each interval.</p> <p>Parameters:</p> Name Type Description Default <code>df_locate</code> <code>DataFrame</code> <p>Input data containing pop-in candidate flags and indentation curve.</p> required <code>popin_flag_column</code> <code>str</code> <p>Column name indicating Boolean pop-in candidate (True/False).</p> <code>'popin'</code> <code>before_window</code> <code>float</code> <p>Time window (in seconds) to use for features before the pop-in event.</p> <code>0.5</code> <code>after_window</code> <code>float</code> <p>Time window (in seconds) to use for features after the pop-in event.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with annotated pop-in intervals and computed statistics (e.g., time, shape, precursor).</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def default_statistics(\n    df_locate, popin_flag_column=\"popin\", before_window=0.5, after_window=0.5\n):\n    \"\"\"\n    Pipeline to compute pop-in statistics from raw located popins.\n\n    This function extracts relevant columns, selects valid pop-in candidates based on local maxima,\n    extracts intervals for each pop-in event, and calculates descriptive statistics for each interval.\n\n    Args:\n        df_locate (pd.DataFrame): Input data containing pop-in candidate flags and indentation curve.\n        popin_flag_column (str): Column name indicating Boolean pop-in candidate (True/False).\n        before_window (float): Time window (in seconds) to use for features before the pop-in event.\n        after_window (float): Time window (in seconds) to use for features after the pop-in event.\n\n    Returns:\n        pd.DataFrame: DataFrame with annotated pop-in intervals and computed statistics (e.g., time, shape, precursor).\n    \"\"\"\n    required_cols = [\"Time (s)\", \"Load (\u00b5N)\", \"Depth (nm)\", popin_flag_column]\n    if \"contact_point\" in df_locate.columns:\n        required_cols.append(\"contact_point\")\n    df_locate = df_locate[required_cols].copy()\n\n    # Postprocess to select local maxima pop-ins\n    df1 = postprocess_popins_local_max(df_locate, popin_flag_column=popin_flag_column)\n\n    # Extract intervals for each pop-in\n    df2 = extract_popin_intervals(df1)\n\n    # Calculate statistics using before_window and after_window\n    return calculate_popin_statistics(\n        df2,\n        time_col=\"Time (s)\",\n        before_window=before_window,  # Pass before_window to the function\n        after_window=after_window,  # Pass after_window to the function\n    )\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.default_statistics_stress_strain","title":"<code>default_statistics_stress_strain(df_locate, popin_flag_column='popin', before_window=0.5, after_window=0.5, Reff_um=5.323, min_load_uN=2000, smooth_stress=True, stress_col='stress', strain_col='strain', time_col='Time (s)')</code>","text":"<p>Full pipeline: from raw data to stress\u2013strain statistics.</p> <p>This includes: - Load\u2013depth pop-in detection - Interval extraction - Stress\u2013strain transformation - Stress\u2013strain statistics</p> <p>Parameters:</p> Name Type Description Default <code>df_locate</code> <code>DataFrame</code> <p>Raw indentation data with pop-in flag column.</p> required <code>popin_flag_column</code> <code>str</code> <p>Column with Boolean flags for pop-in candidates.</p> <code>'popin'</code> <code>before_window</code> <code>float</code> <p>Time window (in seconds) for computing precursor features.</p> <code>0.5</code> <code>after_window</code> <code>float</code> <p>Time window (in seconds) for computing shape-based features.</p> <code>0.5</code> <code>Reff_um</code> <code>float</code> <p>Effective tip radius in microns.</p> <code>5.323</code> <code>min_load_uN</code> <code>float</code> <p>Minimum load threshold for stress\u2013strain conversion.</p> <code>2000</code> <code>smooth_stress</code> <code>bool</code> <p>Whether to smooth the stress signal.</p> <code>True</code> <code>stress_col</code> <code>str</code> <p>Column name for stress data.</p> <code>'stress'</code> <code>strain_col</code> <code>str</code> <p>Column name for strain data.</p> <code>'strain'</code> <code>time_col</code> <code>str</code> <p>Column name for time data.</p> <code>'Time (s)'</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with stress-strain statistics and pop-in intervals.</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def default_statistics_stress_strain(\n    df_locate,\n    popin_flag_column=\"popin\",\n    before_window=0.5,\n    after_window=0.5,\n    Reff_um=5.323,\n    min_load_uN=2000,\n    smooth_stress=True,\n    stress_col=\"stress\",\n    strain_col=\"strain\",\n    time_col=\"Time (s)\",\n):\n    \"\"\"\n    Full pipeline: from raw data to stress\u2013strain statistics.\n\n    This includes:\n    - Load\u2013depth pop-in detection\n    - Interval extraction\n    - Stress\u2013strain transformation\n    - Stress\u2013strain statistics\n\n    Args:\n        df_locate (pd.DataFrame): Raw indentation data with pop-in flag column.\n        popin_flag_column (str): Column with Boolean flags for pop-in candidates.\n        before_window (float): Time window (in seconds) for computing precursor features.\n        after_window (float): Time window (in seconds) for computing shape-based features.\n        Reff_um (float): Effective tip radius in microns.\n        min_load_uN (float): Minimum load threshold for stress\u2013strain conversion.\n        smooth_stress (bool): Whether to smooth the stress signal.\n        stress_col (str): Column name for stress data.\n        strain_col (str): Column name for strain data.\n        time_col (str): Column name for time data.\n\n    Returns:\n        pd.DataFrame: DataFrame with stress-strain statistics and pop-in intervals.\n    \"\"\"\n    df_ld = default_statistics(\n        df_locate,\n        popin_flag_column=popin_flag_column,\n        before_window=before_window,\n        after_window=after_window,\n    )\n\n    df_stress = calculate_stress_strain(\n        df_ld,\n        Reff_um=Reff_um,\n        min_load_uN=min_load_uN,\n        smooth_stress=smooth_stress,\n        copy_popin_cols=True,\n    )\n\n    df_stats = calculate_stress_strain_statistics(\n        df_stress,\n        start_col=\"start_idx\",\n        end_col=\"end_idx\",\n        time_col=time_col,\n        stress_col=stress_col,\n        strain_col=strain_col,\n        before_window=before_window,\n    )\n\n    return df_stats\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.extract_popin_intervals","title":"<code>extract_popin_intervals(df, popin_col='popin_selected', load_col='Load (\u00b5N)')</code>","text":"<p>Extract start and end indices for each pop-in event.</p> <p>For each detected pop-in, this function identifies the start and end points based on the load curve. The start of a pop-in is where the load first increases, and the end is when the load returns to baseline.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with pop-in flags.</p> required <code>popin_col</code> <code>str</code> <p>The column with Boolean values indicating pop-in events.</p> <code>'popin_selected'</code> <code>load_col</code> <code>str</code> <p>The load column used to identify the recovery point.</p> <code>'Load (\u00b5N)'</code> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame with added start and end index columns for each pop-in interval.</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def extract_popin_intervals(df, popin_col=\"popin_selected\", load_col=\"Load (\u00b5N)\"):\n    \"\"\"\n    Extract start and end indices for each pop-in event.\n\n    For each detected pop-in, this function identifies the start and end points based on the load curve.\n    The start of a pop-in is where the load first increases, and the end is when the load returns to baseline.\n\n    Args:\n        df (pd.DataFrame): DataFrame with pop-in flags.\n        popin_col (str): The column with Boolean values indicating pop-in events.\n        load_col (str): The load column used to identify the recovery point.\n\n    Returns:\n        pd.DataFrame: DataFrame with added start and end index columns for each pop-in interval.\n    \"\"\"\n    start_idx_col = [None] * len(df)\n    end_idx_col = [None] * len(df)\n    popin_indices = df.index[df[popin_col]].tolist()\n\n    for start_idx in popin_indices:\n        load_start = df.at[start_idx, load_col]\n        end_idx = start_idx\n        for i in range(start_idx + 1, len(df)):\n            if df.at[i, load_col] &gt;= load_start:\n                end_idx = i\n                break\n        start_idx_col[start_idx] = start_idx\n        end_idx_col[start_idx] = end_idx\n\n    df = df.copy()\n    df[\"start_idx\"] = start_idx_col\n    df[\"end_idx\"] = end_idx_col\n    return df\n</code></pre>"},{"location":"reference/merrypopins.statistics/#merrypopins.statistics.postprocess_popins_local_max","title":"<code>postprocess_popins_local_max(df, popin_flag_column='popin', window=1)</code>","text":"<p>Select pop-ins that have a local load maxima.</p> <p>This function filters out pop-in events that do not represent local maxima in the load curve. A local maximum is defined as a point where the load is higher than the adjacent points within a sliding window.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input indentation data with a pop-in flag column.</p> required <code>popin_flag_column</code> <code>str</code> <p>The column that marks pop-in candidates (True/False).</p> <code>'popin'</code> <code>window</code> <code>int</code> <p>The local window size to assess if the current load is a maximum.</p> <code>1</code> <p>Returns:</p> Type Description <p>pd.DataFrame: The original DataFrame with a new column indicating selected pop-ins.</p> Source code in <code>src/merrypopins/statistics.py</code> <pre><code>def postprocess_popins_local_max(df, popin_flag_column=\"popin\", window=1):\n    \"\"\"\n    Select pop-ins that have a local load maxima.\n\n    This function filters out pop-in events that do not represent local maxima in the load curve.\n    A local maximum is defined as a point where the load is higher than the adjacent points\n    within a sliding window.\n\n    Args:\n        df (pd.DataFrame): Input indentation data with a pop-in flag column.\n        popin_flag_column (str): The column that marks pop-in candidates (True/False).\n        window (int): The local window size to assess if the current load is a maximum.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with a new column indicating selected pop-ins.\n    \"\"\"\n    df = df.copy()\n    max_load_idx = df[\"Load (\u00b5N)\"].idxmax()\n    popin_flags = df[popin_flag_column]\n    selected_indices = []\n\n    for idx in df.index[window:-window]:\n        if idx &gt;= max_load_idx:\n            break\n        if not popin_flags.loc[idx]:\n            continue\n        prev_load = df.at[idx - window, \"Load (\u00b5N)\"]\n        curr_load = df.at[idx, \"Load (\u00b5N)\"]\n        next_load = df.at[idx + window, \"Load (\u00b5N)\"]\n        if curr_load &gt; prev_load and curr_load &gt; next_load:\n            selected_indices.append(idx)\n\n    df[\"popin_selected\"] = False\n    df.loc[selected_indices, \"popin_selected\"] = True\n    logger.info(\n        f\"Filtered to {len(selected_indices)} local max pop-ins before max load\"\n    )\n    return df\n</code></pre>"}]}